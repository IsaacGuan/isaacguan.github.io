<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Isaac&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://isaacguan.github.io/"/>
  <updated>2020-05-15T18:36:42.869Z</updated>
  <id>https://isaacguan.github.io/</id>
  
  <author>
    <name>Yanran Guan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>3D Shape Context for Instance Segmentation</title>
    <link href="https://isaacguan.github.io/2020/05/15/3D-Shape-Context-for-Instance-Segmentation/"/>
    <id>https://isaacguan.github.io/2020/05/15/3D-Shape-Context-for-Instance-Segmentation/</id>
    <published>2020-05-15T17:49:08.000Z</published>
    <updated>2020-05-15T18:36:42.869Z</updated>
    
    <content type="html"><![CDATA[<p>While many deep learning-based methods have been developed to enable 3D instance segmentation, such as <a href="https://github.com/laughtervv/SGPN" target="_blank" rel="external">SGPN</a> and <a href="https://github.com/WXinlong/ASIS" target="_blank" rel="external">ASIS</a>, it is still interesting to see how some traditional methods can perform on this task, for example, using <a href="https://link.springer.com/chapter/10.1007/978-3-540-24672-5_18" target="_blank" rel="external">3D shape contexts</a>. The <a href="https://en.wikipedia.org/wiki/Shape_context" target="_blank" rel="external">shape context</a> is a kind of regional point descriptor that measures the similarity between shapes by calculating the point-wise correspondences between a query shape and a reference shape, and the 3D shape context is its extension to 3D shapes.</p><p>The basic idea of doing instance segmentation leveraging 3D shape contexts is that we iteratively find and remove the best matching shape of each query so as to retrieve the individual instance shapes. This enables a simple form of instance segmentation. For example, using this method, we can retrieve all the instances of straight leg chairs from a room as input, as shown in Figure 1.</p><p><img src="/img/3d_shape_context_for_instance_segmentation/3d_shape_context_for_instance_segmentation_1.jpg" alt="3d shape context for instance segmentation figure 1"></p><center><b>Figure 1:</b> Instances retrieved from an input room, with each marked in a different color.</center><p>In Figure 2 we can take a close look at the retrieved chair instances.</p><p><img src="/img/3d_shape_context_for_instance_segmentation/3d_shape_context_for_instance_segmentation_2.jpg" alt="3d shape context for instance segmentation figure 2"></p><center><b>Figure 1:</b> Retrieved chair instances.</center><p>However, this method is pretty time-consuming, especially when retrieving instances from large input scenes.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;While many deep learning-based methods have been developed to enable 3D instance segmentation, such as &lt;a href=&quot;https://github.com/laught
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="shape-context" scheme="https://isaacguan.github.io/tags/shape-context/"/>
    
      <category term="instance-segmentation" scheme="https://isaacguan.github.io/tags/instance-segmentation/"/>
    
  </entry>
  
  <entry>
    <title>Dealing with Class Imbalance</title>
    <link href="https://isaacguan.github.io/2019/08/10/Dealing-with-Class-Imbalance/"/>
    <id>https://isaacguan.github.io/2019/08/10/Dealing-with-Class-Imbalance/</id>
    <published>2019-08-10T22:21:22.000Z</published>
    <updated>2019-08-17T20:07:39.735Z</updated>
    
    <content type="html"><![CDATA[<p>Class imbalance is a common problem in machine learning, where the total number of one class of data is far less than that of another class of data. Class imbalance affects the quality and reliability of results in machine learning tasks as most of the evaluation metrics assume a balanced class distribution. In this post, I am going to share a few simple yet effective methods that help handle imbalanced datasets, using R language.</p><h1 id="Weighting"><a href="#Weighting" class="headerlink" title="Weighting"></a>Weighting</h1><p>The most regular method is to assign weights to each class. Considering a dataset named <code>data_training</code> with the target variable <code>target</code> as a binary variable and with <code>Y</code> being positive and <code>N</code> being negative, the class weight assignment can be written as follows.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">weights &lt;- ifelse(data_training$target == <span class="string">"Y"</span>,</div><div class="line">                  (<span class="number">1</span>/table(data_training$target)[<span class="number">1</span>]),</div><div class="line">                  (<span class="number">1</span>/table(data_training$target)[<span class="number">2</span>]))</div></pre></td></tr></table></figure><h1 id="Resampling"><a href="#Resampling" class="headerlink" title="Resampling"></a>Resampling</h1><p>The simplest data resampling methods are downsampling and upsampling. Both of them are covered by the <a href="http://topepo.github.io/caret/" target="_blank" rel="external">caret</a> package.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(caret)</div></pre></td></tr></table></figure><p>We only need to specify the resampling method in the control object for training. For downsampling:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ctrl &lt;- trainControl(method = <span class="string">"repeatedcv"</span>,</div><div class="line">                     repeats = <span class="number">5</span>,</div><div class="line">                     classProbs = <span class="literal">TRUE</span>,</div><div class="line">                     summaryFunction = twoClassSummary,</div><div class="line">                     sampling = <span class="string">"down"</span>)</div></pre></td></tr></table></figure><p>And for upsampling:</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ctrl &lt;- trainControl(method = <span class="string">"repeatedcv"</span>,</div><div class="line">                     repeats = <span class="number">5</span>,</div><div class="line">                     classProbs = <span class="literal">TRUE</span>,</div><div class="line">                     summaryFunction = twoClassSummary,</div><div class="line">                     sampling = <span class="string">"up"</span>)</div></pre></td></tr></table></figure><p>There are also a few hybrid methods, such as <a href="https://link.springer.com/article/10.1007/s10618-012-0295-5" target="_blank" rel="external">random over-sampling examples (ROSE)</a> and <a href="https://www.jair.org/index.php/jair/article/view/10302" target="_blank" rel="external">synthetic minority over-sampling technique (SMOTE)</a>, which downsample the majority class and synthesize new data points in the minority class. To use ROSE, we need to load the <a href="https://cran.r-project.org/web/packages/ROSE/" target="_blank" rel="external">ROSE</a> package.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(ROSE)</div></pre></td></tr></table></figure><p>And we create a wrapper around the <code>ROSE</code> function.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">rosest &lt;- list(name = <span class="string">"ROSE"</span>,</div><div class="line">               func = <span class="keyword">function</span>(x, y) &#123;</div><div class="line">                 dat &lt;- <span class="keyword">if</span> (is.data.frame(x)) x <span class="keyword">else</span> as.data.frame(x)</div><div class="line">                 dat$.y &lt;- y</div><div class="line">                 dat &lt;- ROSE(.y ~ ., data = dat, hmult.majo = <span class="number">1</span>, hmult.mino = <span class="number">1</span>)$data</div><div class="line">                 list(x = dat[, !grepl(<span class="string">".y"</span>, colnames(dat), fixed = <span class="literal">TRUE</span>)], </div><div class="line">                      y = dat$.y)</div><div class="line">               &#125;,</div><div class="line">               first = <span class="literal">TRUE</span>)</div></pre></td></tr></table></figure><p>We specify the resampling method in the control object.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ctrl &lt;- trainControl(method = <span class="string">"repeatedcv"</span>,</div><div class="line">                     repeats = <span class="number">5</span>,</div><div class="line">                     classProbs = <span class="literal">TRUE</span>,</div><div class="line">                     summaryFunction = twoClassSummary,</div><div class="line">                     sampling = rosest)</div></pre></td></tr></table></figure><p>Similarly, to use SMOTE, we first load the <a href="https://cran.r-project.org/web/packages/DMwR/" target="_blank" rel="external">DMwR</a> package.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">library</span>(DMwR)</div></pre></td></tr></table></figure><p>Then we create a wrapper around the <code>SMOTE</code> function.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line">smotest &lt;- list(name = <span class="string">"SMOTE"</span>,</div><div class="line">                func = <span class="keyword">function</span>(x, y) &#123;</div><div class="line">                  dat &lt;- <span class="keyword">if</span> (is.data.frame(x)) x <span class="keyword">else</span> as.data.frame(x)</div><div class="line">                  dat$.y &lt;- y</div><div class="line">                  dat &lt;- SMOTE(.y ~ ., data = dat, perc.over = <span class="number">100</span>, k = <span class="number">5</span>)</div><div class="line">                  list(x = dat[, !grepl(<span class="string">".y"</span>, colnames(dat), fixed = <span class="literal">TRUE</span>)], </div><div class="line">                       y = dat$.y)</div><div class="line">                &#125;,</div><div class="line">                first = <span class="literal">TRUE</span>)</div></pre></td></tr></table></figure><p>Finally, we specify the resampling method in the control object.</p><figure class="highlight r"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">ctrl &lt;- trainControl(method = <span class="string">"repeatedcv"</span>,</div><div class="line">                     repeats = <span class="number">5</span>,</div><div class="line">                     classProbs = <span class="literal">TRUE</span>,</div><div class="line">                     summaryFunction = twoClassSummary,</div><div class="line">                     sampling = smotest)</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Class imbalance is a common problem in machine learning, where the total number of one class of data is far less than that of another cla
      
    
    </summary>
    
    
      <category term="data-preprocessing" scheme="https://isaacguan.github.io/tags/data-preprocessing/"/>
    
      <category term="class-imbalance" scheme="https://isaacguan.github.io/tags/class-imbalance/"/>
    
  </entry>
  
  <entry>
    <title>From Shanghai to Xiamen: Biking along the South-East Coast of China</title>
    <link href="https://isaacguan.github.io/2018/07/24/From-Shanghai-to-Xiamen-Biking-along-the-South-East-Coast-of-China/"/>
    <id>https://isaacguan.github.io/2018/07/24/From-Shanghai-to-Xiamen-Biking-along-the-South-East-Coast-of-China/</id>
    <published>2018-07-24T23:12:28.000Z</published>
    <updated>2018-09-24T19:12:45.520Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>We travel just to travel.</p></blockquote><p>The world looks different when you bike. A car window is more or less a divide between the traveler and the outside world, a motorcycle engine could be too noisy for a person who loves a peaceful trip. Biking, however, makes you taste the true beauty of nature: suffering an exhausting uphill ride and then enjoying the kisses on the wind in a downhill.</p><p>I, as well as my roommate, Zhitong, had decided to take a bicycle trip to celebrate our graduation. We traveled from <a href="https://en.wikipedia.org/wiki/Shanghai" target="_blank" rel="external">Shanghai</a> to <a href="https://en.wikipedia.org/wiki/Xiamen" target="_blank" rel="external">Xiamen</a>, biking around 1200 kilometers in total. The bike is not a fast vehicle that could only travel about 100 kilometers per day, so we were able to fully enjoy the beautiful scenery and experience people’s lives along the way.</p><h1 id="Day-1-Setting-Off"><a href="#Day-1-Setting-Off" class="headerlink" title="Day 1: Setting Off"></a>Day 1: Setting Off</h1><p>We set off on June 3rd, 2017.</p><p><img src="/img/from_shanghai_to_xiamen/d1_1.jpg" alt="from shanghai to xiamen d1 1"></p><p>It was a sunny and cloudless morning. By noon, we reached the province boundary of Shanghai and <a href="https://en.wikipedia.org/wiki/Zhejiang" target="_blank" rel="external">Zhejiang</a>.</p><p><img src="/img/from_shanghai_to_xiamen/d1_2.jpg" alt="from shanghai to xiamen d1 2"></p><p>We went across the <a href="https://en.wikipedia.org/wiki/Grand_Canal_%28China%29" target="_blank" rel="external">Beijing–Hangzhou Grand Canal</a> in the afternoon, which is the oldest and longest artificial river in the world, built over 1400 years ago and running over 1700 kilometers. Today, it is still heavily used by shipping.</p><p><img src="/img/from_shanghai_to_xiamen/d1_3.jpg" alt="from shanghai to xiamen d1 3"></p><h1 id="Day-2-Visiting-Hangzhou"><a href="#Day-2-Visiting-Hangzhou" class="headerlink" title="Day 2: Visiting Hangzhou"></a>Day 2: Visiting Hangzhou</h1><p><a href="https://en.wikipedia.org/wiki/Hangzhou" target="_blank" rel="external">Hangzhou</a>, the capital city of Zhejiang Province, is one of the most visited cities in China, renowned for its historic relics as well as natural beauty. We arrived in Hangzhou on the previous night. Although both of us have been to this city before, we still took one rest day here and visited a few interesting places. Being an amateur in <a href="https://en.wikipedia.org/wiki/Chinese_calligraphy" target="_blank" rel="external">Chinese calligraphy</a>, the <a href="https://en.wikipedia.org/wiki/Xiling_Seal_Art_Society" target="_blank" rel="external">Xiling Seal Art Society</a>, though little known by tourists, is one of my favorite spots in Hangzhou. It was founded over 100 years ago and is located in a garden on an island in the north of <a href="https://en.wikipedia.org/wiki/West_Lake" target="_blank" rel="external">West Lake</a>, where we could find a lot of art pieces of Chinese calligraphy and <a href="https://en.wikipedia.org/wiki/Seal_carving" target="_blank" rel="external">seal carving</a>. The picture below was taken inside the Xiling Seal Art Society. Written on the rock are two Chinese characters “闲泉” (read <em>xiánquán</em>) in <a href="https://en.wikipedia.org/wiki/Seal_script" target="_blank" rel="external">seal script</a>, meaning free spring water.</p><p><img src="/img/from_shanghai_to_xiamen/d2_1.jpg" alt="from shanghai to xiamen d2 1"></p><p>For sure we also walked by the West Lake, the most tourist attractive place in Hangzhou. </p><p><img src="/img/from_shanghai_to_xiamen/d2_2.jpg" alt="from shanghai to xiamen d2 2"></p><h1 id="Day-3-Farewell-to-Urban-Life"><a href="#Day-3-Farewell-to-Urban-Life" class="headerlink" title="Day 3: Farewell to Urban Life"></a>Day 3: Farewell to Urban Life</h1><p>It was raining. Before setting out early in the morning, I took a photo of the lovely hostel that we stayed at in Hangzhou.</p><p><img src="/img/from_shanghai_to_xiamen/d3_1.jpg" alt="from shanghai to xiamen d3 1"></p><p>It kept raining. This made me feel that I was boating rather than biking.</p><p><img src="/img/from_shanghai_to_xiamen/d3_2.jpg" alt="from shanghai to xiamen d3 2"></p><p>Heading south all the way, we were gradually getting away from the <a href="https://en.wikipedia.org/wiki/Yangtze_River_Delta" target="_blank" rel="external">Yangtze River Delta</a>, which is dominated by modern cities, and coming into the country, where we could find traditional things such as this arch.</p><p><img src="/img/from_shanghai_to_xiamen/d3_3.jpg" alt="from shanghai to xiamen d3 3"></p><p>Leaving the city, mountains were ahead of us.</p><p><img src="/img/from_shanghai_to_xiamen/d3_4.jpg" alt="from shanghai to xiamen d3 4"></p><h1 id="Day-4-Hengdian-Chinawood-in-the-Mountains"><a href="#Day-4-Hengdian-Chinawood-in-the-Mountains" class="headerlink" title="Day 4: Hengdian, Chinawood in the Mountains"></a>Day 4: Hengdian, Chinawood in the Mountains</h1><p>We arrived in <a href="https://en.wikipedia.org/wiki/Yiwu" target="_blank" rel="external">Yiwu</a> the previous night. However, I was struck by an <a href="https://en.wikipedia.org/wiki/Iliotibial_band_syndrome" target="_blank" rel="external">IT band pain</a> and couldn’t ride a long distance the next day. So on day 4, we decided to make a short detour and ride to <a href="https://en.wikipedia.org/wiki/Hengdian_World_Studios" target="_blank" rel="external">Hengdian Town</a> only, which is about 30 kilometers southeast from Yiwu and where the largest film studio in China is located (so it is called <em>Chinawood</em>).</p><p><img src="/img/from_shanghai_to_xiamen/d4_1.jpg" alt="from shanghai to xiamen d4 1"></p><p>Every year, tens of thousands of amateur actors/actresses come to this small town, playing as <a href="https://en.wikipedia.org/wiki/Extra_%28acting%29" target="_blank" rel="external">extras</a> in movies or TV series, dreaming of becoming movie stars someday, though the chance is very little. These amateur actors/actresses almost take up half of the population in Hengdian and you come across them almost everywhere in the town. As you see, in the picture, an amateur actor was practicing his script.</p><p><img src="/img/from_shanghai_to_xiamen/d4_2.jpg" alt="from shanghai to xiamen d4 2"></p><p>Being passing travelers, we were not really interested in the film making things. We climbed up a hill and enjoyed the fascinating scenery of a Chinese mountain town in the sunset.</p><p><img src="/img/from_shanghai_to_xiamen/d4_3.jpg" alt="from shanghai to xiamen d4 3"></p><p>And we walked through the pedestrian street of Hengdian in the evening.</p><p><img src="/img/from_shanghai_to_xiamen/d4_4.jpg" alt="from shanghai to xiamen d4 4"></p><h1 id="Day-5-Rivers-amp-Mountains"><a href="#Day-5-Rivers-amp-Mountains" class="headerlink" title="Day 5: Rivers &amp; Mountains"></a>Day 5: Rivers &amp; Mountains</h1><p>We left Hengdian early in the morning, as the destination of this day was <a href="https://en.wikipedia.org/wiki/Lishui" target="_blank" rel="external">Lishui</a>, which is about 120 kilometers away. At the time of departure, I saw this sign, which is the title of the drama movie <a href="https://en.wikipedia.org/wiki/I_Am_Somebody" target="_blank" rel="external"><em>I Am Somebody</em></a>, a movie about extras working in Hengdian.</p><p><img src="/img/from_shanghai_to_xiamen/d5_1.jpg" alt="from shanghai to xiamen d5 1"></p><p>We passed by a small village, Baita Village, in the morning, where villagers were making straw mats in a traditional manner. An old lady was using a big fan to blow away those defective straws.</p><p><img src="/img/from_shanghai_to_xiamen/d5_2.jpg" alt="from shanghai to xiamen d5 2"></p><p>The straws need to be dried in the sun.</p><p><img src="/img/from_shanghai_to_xiamen/d5_3.jpg" alt="from shanghai to xiamen d5 3"></p><p>South Zhejiang is a mountainous region where a lot of rivers flow through. People here build houses either by the river,</p><p><img src="/img/from_shanghai_to_xiamen/d5_4.jpg" alt="from shanghai to xiamen d5 4"></p><p>or by the road. And usually, the houses are on the mountain, nestling against the slope.</p><p><img src="/img/from_shanghai_to_xiamen/d5_5.jpg" alt="from shanghai to xiamen d5 5"></p><p>We biked through <a href="https://en.wikipedia.org/wiki/Jinyun_County" target="_blank" rel="external">Jinyun County</a> in the afternoon, which is famous for its pancake. I tried one, taking me only 5 <a href="https://en.wikipedia.org/wiki/Renminbi" target="_blank" rel="external">yuan</a>. It is filled up with meat, very juicy and tasty.</p><p><img src="/img/from_shanghai_to_xiamen/d5_6.jpg" alt="from shanghai to xiamen d5 6"></p><p>The national road in Zhejiang is really in nice condition. Pretty enjoyable ride!</p><p><img src="/img/from_shanghai_to_xiamen/d5_7.jpg" alt="from shanghai to xiamen d5 7"></p><p>We arrived in Lishui in the evening. Lishui is a small city and a bit undeveloped. This makes some old things better preserved, for example, a busy <a href="https://en.wikipedia.org/wiki/Night_market" target="_blank" rel="external">night market</a>, where you could always make good bargains, but now hard to find in big cities.</p><p><img src="/img/from_shanghai_to_xiamen/d5_8.jpg" alt="from shanghai to xiamen d5 8"></p><h1 id="Day-6-Chased-by-a-Rainstorm"><a href="#Day-6-Chased-by-a-Rainstorm" class="headerlink" title="Day 6: Chased by a Rainstorm"></a>Day 6: Chased by a Rainstorm</h1><p>Leaving Lishui, immediately we rode into the mountains. The weather was good in the morning when we were riding on a riverside road. </p><p><img src="/img/from_shanghai_to_xiamen/d6_1.jpg" alt="from shanghai to xiamen d6 1"></p><p>At around 3:00 PM, thunder rumbled from a distance, cool winds blew from the back. When turning around, I saw dark clouds gathering, indicating that a thunderstorm was coming, also indicating that we need some more speed.</p><p><img src="/img/from_shanghai_to_xiamen/d6_2.jpg" alt="from shanghai to xiamen d6 2"></p><p>Apparently, nobody can ride faster than winds and clouds. I took shelter in a roadside village when it started to rain, finding this <a href="https://en.wikipedia.org/wiki/Shigandang" target="_blank" rel="external">Shigandang</a> tablet at the entrance of the village.</p><p><img src="/img/from_shanghai_to_xiamen/d6_3.jpg" alt="from shanghai to xiamen d6 3"></p><p>After the rain, it was pleasant to bike in the mist-shrouded mountains.</p><p><img src="/img/from_shanghai_to_xiamen/d6_4.jpg" alt="from shanghai to xiamen d6 4"></p><p>In the evening, we came to <a href="https://en.wikipedia.org/wiki/Jingning_She_Autonomous_County" target="_blank" rel="external">Jingning County</a>, which is an autonomous county for the <a href="https://en.wikipedia.org/wiki/She_people" target="_blank" rel="external">She people</a>. We tried some unique dishes, for example, the green Tofu, which is actually a kind of Tofu-like food made from <a href="https://en.wikipedia.org/wiki/Premna" target="_blank" rel="external">Premna</a> leaves.</p><p><img src="/img/from_shanghai_to_xiamen/d6_5.jpg" alt="from shanghai to xiamen d6 5"></p><p>Besides, we found this interesting phenomenon: the smog rising on the light was actually caused by the raindrop falling on it.</p><p><img src="/img/from_shanghai_to_xiamen/d6_6.jpg" alt="from shanghai to xiamen d6 6"></p><h1 id="Day-7-Biking-through-the-Mountains"><a href="#Day-7-Biking-through-the-Mountains" class="headerlink" title="Day 7: Biking through the Mountains"></a>Day 7: Biking through the Mountains</h1><p>Jingning lies in a valley surrounded by mountains. This indicates a long uphill ride in front of us.</p><p><img src="/img/from_shanghai_to_xiamen/d7_1.jpg" alt="from shanghai to xiamen d7 1"></p><p>There are wild animals in the mountains. The sign in the picture warns people against feeding the monkeys passing by.</p><p><img src="/img/from_shanghai_to_xiamen/d7_2.jpg" alt="from shanghai to xiamen d7 2"></p><p>There are springs in the mountains as well. Although it is unsafe to drink spring water directly, we still tried a bit. The water tastes cold and sweet.</p><p><img src="/img/from_shanghai_to_xiamen/d7_3.jpg" alt="from shanghai to xiamen d7 3"></p><p>We reached the mountaintop in the afternoon, where there is a tunnel, and the altitude is over 1000m, the highest place we arrived during the trip. The sign by the tunnel entrance says that the continuous steep downhill ahead has caused 25 deaths.</p><p><img src="/img/from_shanghai_to_xiamen/d7_4.jpg" alt="from shanghai to xiamen d7 4"></p><p>During the downhill ride, we experienced the best mountain views throughout the trip:</p><p><img src="/img/from_shanghai_to_xiamen/d7_5.jpg" alt="from shanghai to xiamen d7 5"></p><p><img src="/img/from_shanghai_to_xiamen/d7_6.jpg" alt="from shanghai to xiamen d7 6"></p><p><img src="/img/from_shanghai_to_xiamen/d7_7.jpg" alt="from shanghai to xiamen d7 7"></p><p><img src="/img/from_shanghai_to_xiamen/d7_8.jpg" alt="from shanghai to xiamen d7 8"></p><p><img src="/img/from_shanghai_to_xiamen/d7_9.jpg" alt="from shanghai to xiamen d7 9"></p><p>as well as lovely wildflowers in the sunset.</p><p><img src="/img/from_shanghai_to_xiamen/d7_10.jpg" alt="from shanghai to xiamen d7 10"></p><p>However, Zhitong’s bike got punctured. It took us some time to fix the tube.</p><p><img src="/img/from_shanghai_to_xiamen/d7_11.jpg" alt="from shanghai to xiamen d7 11"></p><p>Due to the issue of Zhitong’s bike, we couldn’t ride further and had to take a rest in Siqian Town, a small town in <a href="https://en.wikipedia.org/wiki/Taishun_County" target="_blank" rel="external">Taishun County</a>, lying in the foothills of the mountains. Fortunately, we met a man in the town, Zhangqiang, who used to travel around China by bike in 2010, raised funds while traveling, and donated the money for needy students in the mountains. We could even find <a href="http://www.wzrb.com.cn/article135850_256show.html" target="_blank" rel="external">a news report about him (in Chinese)</a>. He helped us repair the bike and treated us to dinner. We greatly appreciated his help!</p><p><img src="/img/from_shanghai_to_xiamen/d7_12.jpg" alt="from shanghai to xiamen d7 12"></p><h1 id="Day-8-Coming-into-Fujian-Province"><a href="#Day-8-Coming-into-Fujian-Province" class="headerlink" title="Day 8: Coming into Fujian Province"></a>Day 8: Coming into Fujian Province</h1><p>We stayed in the small town for one night and set off the next morning, it was a sunny day.</p><p><img src="/img/from_shanghai_to_xiamen/d8_1.jpg" alt="from shanghai to xiamen d8 1"></p><p>A Double waterfall by the road. Local people call the falls <em>Hongyan Double Falls</em> or <em>the Couple Falls</em>.</p><p><img src="/img/from_shanghai_to_xiamen/d8_2.jpg" alt="from shanghai to xiamen d8 2"></p><p>Zhitong’s bike was still not in good condition, so we found a bike repair shop in Taishun, and had the bike fixed. </p><p><img src="/img/from_shanghai_to_xiamen/d8_3.jpg" alt="from shanghai to xiamen d8 3"></p><p>The boundary line between Zhejiang and <a href="https://en.wikipedia.org/wiki/Fujian" target="_blank" rel="external">Fujian</a> lies only about 5 kilometers south to Taishun. We officially came into Fujian Province in the afternoon. I have a photo of me standing by the cross-boundary bridge at the boundary line.</p><p><img src="/img/from_shanghai_to_xiamen/d8_4.jpg" alt="from shanghai to xiamen d8 4"></p><p>It seems like that some Chinese customs and traditions are better preserved in Fujian, for example, an <a href="https://en.wikipedia.org/wiki/Ancestral_shrine" target="_blank" rel="external">ancestral shrine</a>, where people perform ancestor worship.</p><p><img src="/img/from_shanghai_to_xiamen/d8_5.jpg" alt="from shanghai to xiamen d8 5"></p><p>And there is also an interesting old house. </p><p><img src="/img/from_shanghai_to_xiamen/d8_6.jpg" alt="from shanghai to xiamen d8 6"></p><p>An old peasant.</p><p><img src="/img/from_shanghai_to_xiamen/d8_7.jpg" alt="from shanghai to xiamen d8 7"></p><p>A town in the valley, which was very quiet and almost empty. We conjectured that young people had basically left such remote mountain towns for cities to either pursue their study or work.</p><p><img src="/img/from_shanghai_to_xiamen/d8_8.jpg" alt="from shanghai to xiamen d8 8"></p><p>Look back down the valley and the town that we had passed by.</p><p><img src="/img/from_shanghai_to_xiamen/d8_9.jpg" alt="from shanghai to xiamen d8 9"></p><p>A small brick kiln. The red clay here in the mountains is very suitable for brick making.</p><p><img src="/img/from_shanghai_to_xiamen/d8_10.jpg" alt="from shanghai to xiamen d8 10"></p><h1 id="Day-9-A-Super-Hot-Day"><a href="#Day-9-A-Super-Hot-Day" class="headerlink" title="Day 9: A Super Hot Day"></a>Day 9: A Super Hot Day</h1><p>We stayed the previous night in <a href="https://en.wikipedia.org/wiki/Nanyang,_Shouning_County" target="_blank" rel="external">Nanyang Town</a>, a small mountain town. In the morning, we started down the road by a mountain stream. It was downhill all the way. People built a small dam on the stream.</p><p><img src="/img/from_shanghai_to_xiamen/d9_1.jpg" alt="from shanghai to xiamen d9 1"></p><p>The dam does not let too much water through.</p><p><img src="/img/from_shanghai_to_xiamen/d9_2.jpg" alt="from shanghai to xiamen d9 2"></p><p>We came to another mountain town, Wuqu Town. Written on the door was a piece of military conscription, in pretty good handwriting.</p><p><img src="/img/from_shanghai_to_xiamen/d9_3.jpg" alt="from shanghai to xiamen d9 3"></p><p>The sun was shining fiercely, so we took a short rest in the town, and tried some Bian Rou, a kind of Fujianese style <a href="https://en.wikipedia.org/wiki/Wonton" target="_blank" rel="external">wonton</a>, over a roadside snack stall.</p><p><img src="/img/from_shanghai_to_xiamen/d9_4.jpg" alt="from shanghai to xiamen d9 4"></p><p>Due to the hot weather, we couldn’t ride for a very long distance this day and finally arrived in Saiqi, a town of <a href="https://en.wikipedia.org/wiki/Fu%27an" target="_blank" rel="external">Fu’an</a>. People in Fujian have their own unique religious belief, the worship of <a href="https://en.wikipedia.org/wiki/Mazu" target="_blank" rel="external">Mazu</a>. We saw this Mazu temple on our way.</p><p><img src="/img/from_shanghai_to_xiamen/d9_5.jpg" alt="from shanghai to xiamen d9 5"></p><h1 id="Day-10-Ningde-the-First-Seaside-City-of-Our-Trip"><a href="#Day-10-Ningde-the-First-Seaside-City-of-Our-Trip" class="headerlink" title="Day 10: Ningde, the First Seaside City of Our Trip"></a>Day 10: Ningde, the First Seaside City of Our Trip</h1><p>It was a cloudy day, we still had to ride in the mountains before we reached <a href="https://en.wikipedia.org/wiki/Ningde" target="_blank" rel="external">Ningde</a>, the northernmost seaside city of Fujian. There is a converting station in the mountains.</p><p><img src="/img/from_shanghai_to_xiamen/d10_1.jpg" alt="from shanghai to xiamen d10 1"></p><p>The advertisement on the wall says that this guy sells guns. Considering the extremely strict gun control policy in China, such advertisements are probably deceptive, which means you won’t get anything after paying him money.</p><p><img src="/img/from_shanghai_to_xiamen/d10_2.jpg" alt="from shanghai to xiamen d10 2"></p><p>A small waterfall flowing between mountains.</p><p><img src="/img/from_shanghai_to_xiamen/d10_3.jpg" alt="from shanghai to xiamen d10 3"></p><p>Crossing the bridge, we came into Ningde. On the bridge it writes a verse of <a href="https://en.wikipedia.org/wiki/Mao_Zedong" target="_blank" rel="external">Chairman Mao</a>: “虎距龙盘今胜昔, 天翻地覆慨而慷”, translated as <em>The city, a tiger crouching, a dragon curling, outshines its ancient glories; In heroic triumph heaven and earth have been overturned</em>.</p><p><img src="/img/from_shanghai_to_xiamen/d10_4.jpg" alt="from shanghai to xiamen d10 4"></p><h1 id="Day-11-Seeing-the-Sea-Finally"><a href="#Day-11-Seeing-the-Sea-Finally" class="headerlink" title="Day 11: Seeing the Sea, Finally"></a>Day 11: Seeing the Sea, Finally</h1><p>We planned to get to <a href="https://en.wikipedia.org/wiki/Fuzhou" target="_blank" rel="external">Fuzhou</a> this day. According to the map, the sea was only a few kilometers away from us. However, there were still quite a few mountains on our way.</p><p><img src="/img/from_shanghai_to_xiamen/d11_1.jpg" alt="from shanghai to xiamen d11 1"></p><p>From the mountain road, we could see a factory by the sea.</p><p><img src="/img/from_shanghai_to_xiamen/d11_2.jpg" alt="from shanghai to xiamen d11 2"></p><p>And finally, we reached a coastal road, although the bay by the road was not beautiful.</p><p><img src="/img/from_shanghai_to_xiamen/d11_3.jpg" alt="from shanghai to xiamen d11 3"></p><h1 id="Day-12-Visiting-Fuzhou"><a href="#Day-12-Visiting-Fuzhou" class="headerlink" title="Day 12: Visiting Fuzhou"></a>Day 12: Visiting Fuzhou</h1><p>Fuzhou, the capital city of Fujian Province, famous for the hundreds of big <a href="https://en.wikipedia.org/wiki/Banyan" target="_blank" rel="external">Banyan</a> trees planted in the city, has a poetic nickname <em>Rongcheng</em>, meaning the city of Banyan.</p><p><img src="/img/from_shanghai_to_xiamen/d12_1.jpg" alt="from shanghai to xiamen d12 1"></p><p>I was a bit surprised when I saw such a large mosque in Fuzhou, as I had thought everybody here is Mazuist. Actually, this <a href="https://en.wikipedia.org/wiki/Fuzhou_Mosque" target="_blank" rel="external">Fuzhou Mosque</a> was built almost 800 years ago, in <a href="https://en.wikipedia.org/wiki/Yuan_dynasty" target="_blank" rel="external">Yuan dynasty</a>, and is one of the most ancient mosques in China.</p><p><img src="/img/from_shanghai_to_xiamen/d12_2.jpg" alt="from shanghai to xiamen d12 2"></p><p>In the evening, I went to the old alleys of Fuzhou, to experience some traditional atmosphere of the city.</p><p><img src="/img/from_shanghai_to_xiamen/d12_3.jpg" alt="from shanghai to xiamen d12 3"></p><h1 id="Day-13-Setting-Off-Alone"><a href="#Day-13-Setting-Off-Alone" class="headerlink" title="Day 13: Setting Off Alone"></a>Day 13: Setting Off Alone</h1><p>Still having an exam to take, Zhitong had to go back to Shanghai from Fuzhou. I carried on with my trip alone. Xiamen was only 234 kilometers ahead of me.</p><p><img src="/img/from_shanghai_to_xiamen/d13_1.jpg" alt="from shanghai to xiamen d13 1"></p><p>I reached <a href="https://en.wikipedia.org/wiki/Putian" target="_blank" rel="external">Putian</a> by afternoon. It was raining for the whole day.</p><p><img src="/img/from_shanghai_to_xiamen/d13_2.jpg" alt="from shanghai to xiamen d13 2"></p><h1 id="Day-14-Endless-Rain"><a href="#Day-14-Endless-Rain" class="headerlink" title="Day 14: Endless Rain"></a>Day 14: Endless Rain</h1><p>The road was under construction outside Putian, where the road condition was made even worse by the rain.</p><p><img src="/img/from_shanghai_to_xiamen/d14_1.jpg" alt="from shanghai to xiamen d14 1"></p><p>Very interestingly, I came across a Mongolian village. I had thought Mongolians only live in North Asia.</p><p><img src="/img/from_shanghai_to_xiamen/d14_2.jpg" alt="from shanghai to xiamen d14 2"></p><p>I arrived in <a href="https://en.wikipedia.org/wiki/Quanzhou" target="_blank" rel="external">Quanzhou</a> at the end of the day, a city used to be a major port for foreign traders and hometown of <a href="https://en.wikipedia.org/wiki/Overseas_Chinese" target="_blank" rel="external">overseas Chinese</a>, and stayed there for the night. The buildings over there are in a style mixing east with west.</p><p><img src="/img/from_shanghai_to_xiamen/d14_3.jpg" alt="from shanghai to xiamen d14 3"></p><h1 id="Day-15-Arriving-in-Xiamen"><a href="#Day-15-Arriving-in-Xiamen" class="headerlink" title="Day 15: Arriving in Xiamen"></a>Day 15: Arriving in Xiamen</h1><p>Xiamen is only about 90 kilometers from Quanzhou, so this was the last day of my bike tour. There was a big traffic jam on the way to Xiamen.</p><p><img src="/img/from_shanghai_to_xiamen/d15_1.jpg" alt="from shanghai to xiamen d15 1"></p><p>The boundary marker of Xiamen, carrying a lot of names and comments from previous travelers.</p><p><img src="/img/from_shanghai_to_xiamen/d15_2.jpg" alt="from shanghai to xiamen d15 2"></p><p>Due to the rain, the cross-sea bridge to <a href="https://en.wikipedia.org/wiki/Xiamen_Island" target="_blank" rel="external">Xiamen Island</a> was temporarily closed to pedestrians and cyclists. So I had to take a 5-minute shuttle bus ride to get into the city. </p><p><img src="/img/from_shanghai_to_xiamen/d15_3.jpg" alt="from shanghai to xiamen d15 3"></p><p>I met up with this interesting guy on arrival in Xiamen when we were both sheltering from the rain under an overpass. He talked with me about some local customs and traditional worships here. Actually, apart from Mazu, Xiamenese people worship <a href="https://en.wikipedia.org/wiki/Nezha" target="_blank" rel="external">Nezha</a> as well. He took me to a temple of Nezha later and showed me around the temple. </p><p><img src="/img/from_shanghai_to_xiamen/d15_4.jpg" alt="from shanghai to xiamen d15 4"></p><p>The Nezha temple.</p><p><img src="/img/from_shanghai_to_xiamen/d15_5.jpg" alt="from shanghai to xiamen d15 5"></p><h1 id="Day-16-to-Day-18-Visiting-Xiamen"><a href="#Day-16-to-Day-18-Visiting-Xiamen" class="headerlink" title="Day 16 to Day 18: Visiting Xiamen"></a>Day 16 to Day 18: Visiting Xiamen</h1><p>I found a youth hostel on <a href="https://en.wikipedia.org/wiki/Gulangyu" target="_blank" rel="external">Gulangyu</a>, a small and artsy island off the coast of Xiamen, renowned for its varied architecture and multicultural history.</p><p><img src="/img/from_shanghai_to_xiamen/d16_1.jpg" alt="from shanghai to xiamen d16 1"></p><p><img src="/img/from_shanghai_to_xiamen/d16_2.jpg" alt="from shanghai to xiamen d16 2"></p><p><img src="/img/from_shanghai_to_xiamen/d16_3.jpg" alt="from shanghai to xiamen d16 3"></p><p>The next day, I went to the east beach, from where the <a href="https://en.wikipedia.org/wiki/Kinmen" target="_blank" rel="external">Kinmen Islands</a>, which are still governed by the <a href="https://en.wikipedia.org/wiki/Taiwan" target="_blank" rel="external">Republic of China</a>, were within my sight.</p><p><img src="/img/from_shanghai_to_xiamen/d17_1.jpg" alt="from shanghai to xiamen d17 1"></p><p>There is a big propaganda sign on the coast of Xiamen facing the Kinmen Islands, telling the hope of <a href="https://en.wikipedia.org/wiki/Chinese_unification" target="_blank" rel="external">Chinese reunification</a>.</p><p><img src="/img/from_shanghai_to_xiamen/d17_2.jpg" alt="from shanghai to xiamen d17 2"></p><p>I also visited <a href="https://en.wikipedia.org/wiki/Xiamen_University" target="_blank" rel="external">Xiamen University</a> and walked through the <a href="https://www.tripadvisor.com/Attraction_Review-g297407-d6418938-Reviews-Furong_Tunnel-Xiamen_Fujian.html" target="_blank" rel="external">Furong Tunnel</a>, which is filled with graffiti designed by university students.</p><p><img src="/img/from_shanghai_to_xiamen/d17_3.jpg" alt="from shanghai to xiamen d17 3"></p><p>I stayed in this lovely coastal city (though the rainy weather was a bit frustrating) for 3 days, also tried as much local seafood as possible. On the last day of my stay, when taking a ferry from Gulangyu to Xiamen, I took this skyline picture, drawing the modern face of the city. It was the last picture of our trip.</p><p><img src="/img/from_shanghai_to_xiamen/d18_1.jpg" alt="from shanghai to xiamen d18 1"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;We travel just to travel.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The world looks different when you bike. A car window is more or less a divid
      
    
    </summary>
    
    
      <category term="biking" scheme="https://isaacguan.github.io/tags/biking/"/>
    
      <category term="trip" scheme="https://isaacguan.github.io/tags/trip/"/>
    
  </entry>
  
  <entry>
    <title>Plane Detection Using Deep Learning Approach</title>
    <link href="https://isaacguan.github.io/2018/06/01/Plane-Detection-Using-Deep-Learning-Approach/"/>
    <id>https://isaacguan.github.io/2018/06/01/Plane-Detection-Using-Deep-Learning-Approach/</id>
    <published>2018-06-01T15:09:59.000Z</published>
    <updated>2018-06-26T23:39:53.278Z</updated>
    
    <content type="html"><![CDATA[<p>Plane detection is a widely used technique that can be applied in many applications, e.g., augmented reality (AR), where we have to detect a plane to generate AR models, and 3D scene reconstruction, especially for man-made scenes, which consist of many planar objects. Nowadays, with the proliferation of acquisitive devices, deriving a massive point cloud is not a difficult task, which shows promise in doing plane detection in 3D point clouds.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>There are some existing plane detection and point cloud deep learning approaches proposed by recent researches.</p><h2 id="Plane-Detection-in-3D-Data"><a href="#Plane-Detection-in-3D-Data" class="headerlink" title="Plane Detection in 3D Data"></a>Plane Detection in 3D Data</h2><p>The <a href="https://link.springer.com/article/10.1007/3DRes.02%282011%293" target="_blank" rel="external">3D Hough transform</a> is one possible approach for doing plane detection. As well as line detection in 2D space, planes can be parameterized into a 3D Hough space. The <a href="https://github.com/amonszpart/RAPter" target="_blank" rel="external">RAPter</a> is another method that can be used for plane detection. It finds out the planes in a scene according to the predefined inter-plane relations, so RAPter is efficient for man-made scenes with significant inter-plane relations, but not adequate for some more general cases.</p><h2 id="Deep-Learning-for-3D-Data-and-Point-Cloud"><a href="#Deep-Learning-for-3D-Data-and-Point-Cloud" class="headerlink" title="Deep Learning for 3D Data and Point Cloud"></a>Deep Learning for 3D Data and Point Cloud</h2><p>People have designed many deep learning approaches for different representations of 3D data. For instance, the <a href="https://github.com/dimatura/voxnet" target="_blank" rel="external">volumetric CNN</a> consumes volumetric data as input, and apply 3D convolutional neural networks to voxelized shapes. The <a href="https://github.com/suhangpro/mvcnn" target="_blank" rel="external">multiview CNN</a> projects the 3D shape into 2D images, and then apply 2D convolutional neural networks to classify them. The <a href="https://openaccess.thecvf.com/content_cvpr_2015/html/Fang_3D_Deep_Shape_2015_CVPR_paper.html" target="_blank" rel="external">feature-based DNN</a> focuses on generating a shape vector of the object according to its traditional shape features, and then use a fully connected net to classify the shape. The <a href="https://github.com/charlesq34/pointnet" target="_blank" rel="external">PointNet</a> proposed a new design of neural network based on symmetric functions that can take unordered input of point clouds.</p><h1 id="Design-of-Neural-Networks"><a href="#Design-of-Neural-Networks" class="headerlink" title="Design of Neural Networks"></a>Design of Neural Networks</h1><p>PointNet uses symmetric functions that can effectively capture the global features of a point cloud. Inspired by this, this experiment uses a symmetric network that concatenates global features and local features. Besides, for comparison, I also used a traditional network that simply generates a high dimensional local feature space by multilayer perceptions.</p><h2 id="Symmetric-Network"><a href="#Symmetric-Network" class="headerlink" title="Symmetric Network"></a>Symmetric Network</h2><p>According to the universal approximation of symmetric function proposed by PointNet, a symmetric function $f$ can be arbitrarily approximated by a composition of a set of single-variable functions and a max pooling function, as described in Theorem 1.</p><p><strong>Theorem 1:</strong> Suppose $f\colon\chi\to\mathbb{R}$ is a continuous set function w.r.t. Hausdorff distance $d_H$. $\forall\epsilon&gt;0$, $\exists$ a continuous function $h$ and a symmetric function $g(\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n)=\gamma\circ\max$, such that for any $S\in\chi$,$$\begin{equation}|f(S)-\gamma(\max_{\mathbf{x}_i\in S}\{h(\mathbf{x}_i)\})|&lt;\epsilon,\tag{1}\label{eq:1}\end{equation}$$where $\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n$ is the full list of elements in $S$ ordered arbitrarily, $\gamma$ is a continuous function, and $\max$ is a vector max operator that takes $n$ vectors as input and returns a new vector of the element-wise maximum.</p><p>Thus, according to Theorem 1, the symmetric network can be designed as a multi-layer perceptron network connected with a max pooling function, which is shown in Figure 1.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_1.jpg" alt="plane detection using deep learning approach figure 1"></p><center><b>Figure 1:</b> Architecture of the symmetric network.</center><p>In order to achieve the invariance towards geometric transformation, an input alignment ($\mathbf{T}_1$ in Figure 1) and a feature alignment ($\mathbf{T}_2$ in Figure 1) are respectively applied to the input space and feature space. The points are first mapped to a 64-dimensional feature space and then mapped to a 1024-dimensional feature space. A max pooling function is applied to the 1024-dimensional feature space to generate a 1024-length global feature vector. The global vector is then concatenated to the 64-dimensional feature space which generates a 1088-dimensional space. Lastly, a 2-dimensional vector for each point, which represents the score for the planar part and non-planar part, is updated from the 1088-dimensional space.</p><h2 id="Asymmetric-Network"><a href="#Asymmetric-Network" class="headerlink" title="Asymmetric Network"></a>Asymmetric Network</h2><p>For doing a comparison, a traditional asymmetric network is also introduced in this experiment. It is basically modified from the symmetric network by detaching the max pooling function from the multi-layer perceptron network. The architecture of the asymmetric network is shown in Figure 2.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_2.jpg" alt="plane detection using deep learning approach figure 2"></p><center><b>Figure 2:</b> Architecture of the asymmetric network.</center><p>Instead of concatenating the global feature vector to the 64-dimensional feature space, the asymmetric network simply concatenates the 64-dimensional feature space and the 1024-dimensional feature space, and generates the 2-dimensional scores from that.</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>The experiment is conducted in the following pattern: first, prepare the data for training and testing; then feed the training data respectively to the symmetric network and asymmetric network, and find the best-trained model according to the minimum total loss; lastly, compare the plane detection results of symmetric network and asymmetric network.</p><h2 id="Experimental-Data-and-Data-Preprocessing"><a href="#Experimental-Data-and-Data-Preprocessing" class="headerlink" title="Experimental Data and Data Preprocessing"></a>Experimental Data and Data Preprocessing</h2><p>This experiment uses data from the <a href="http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html" target="_blank" rel="external">ShapeNetPart</a> dataset. I chose 64 tables, which have a significant planar surface, from the table repository for training, and 8 for testing and evaluation.</p><p>Each the point cloud was previously undersampled to a size of 2048 points, using a random sample. The point clouds for training were written into an HDF5 file, which contains 2 views, <code>points</code> and <code>pid</code>, respectively recording the point coordinate and the planar information associated with each point.</p><p>The planar parts were marked out manually on the original data. Figure 3 shows a few examples of table objects for training.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_3.jpg" alt="plane detection using deep learning approach figure 3"></p><center><b>Figure 3:</b> A part of the training data.</center><h2 id="Best-Trained-Models"><a href="#Best-Trained-Models" class="headerlink" title="Best Trained Models"></a>Best Trained Models</h2><p>Basically, a point cloud contains more points in the non-planar part than points in the planar part. In order to handle this unbalanced data, I used a weighted cross-entropy function to calculate the mean loss for each epoch. The planar part is assigned with a weight of 0.7 and the non-planar part is assigned with a weight of 0.3.</p><p>Both networks are trained for 150 epochs. The plot of mean loss for training the symmetric network is shown in Figure 4.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_4.jpg" alt="plane detection using deep learning approach figure 4"></p><center><b>Figure 4:</b> Total mean loss per epoch for the training of symmetric network.</center><p>According to Figure 4, the total mean loss is stuck at a very low value after the 100th epoch, so I chose the trained model from the 130th epoch for testing.</p><p>Besides, Figure 5 is the plot of mean loss for training the asymmetric network.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_5.jpg" alt="plane detection using deep learning approach figure 5"></p><center><b>Figure 5:</b> Total mean loss per epoch for the training of asymmetric network.</center><p>The loss value stays low after the 100th epoch, and there is a fluctuation around the 130th epoch. Such fluctuation may be caused by overfitting, so I chose the trained model from the 110th epoch for testing.</p><h2 id="Experiment-Results"><a href="#Experiment-Results" class="headerlink" title="Experiment Results"></a>Experiment Results</h2><p>The test set has a size of 8 objects. The test result for the model of the symmetric network shows an accuracy of 83.4534% and an <a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="external">Intersection over Union (IoU)</a> of 71.1421%. Figure 6 illustrates the plane detection result in the test set using the model generated by the symmetric network.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_6.jpg" alt="plane detection using deep learning approach figure 6"></p><center><b>Figure 6:</b> Test results of the symmetric network.</center><p>The result shows a fairly good performance of neural networks doing plane detection for objects from a single category. The accuracy could even possibly rise if a larger training set is prepared. The model seems to favor a table object with a more normal shape, i.e., a table with a square tabletop and four straight legs. For tables without a regular shape, the classification accuracy is relatively lower, and the model tends to misclassify the points in the middle of the tabletop.</p><p>The asymmetric network shows a similar test result, which comes out with an accuracy of 85.7117% and an IoU of 75.0279%. The plane detection results of the asymmetric network are shown in Figure 7 below.</p><p><img src="/img/plane_detection_using_deep_learning_approach/plane_detection_using_deep_learning_approach_7.jpg" alt="plane detection using deep learning approach figure 7"></p><center><b>Figure 7:</b> Test results of the symmetric network.</center><p>Similar to the results of the symmetric network, the model based on the asymmetric network shows a good performance on objects with a more regular shape. It may also fail to classify the points in the middle of the tabletop. Furthermore, the asymmetric network could also misclassify a few points on the legs of a table, which is shown in the 1st, 2nd, and 8th objects, and such a pattern is not observed in the results of the symmetric network.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this experiment, although the asymmetric network shows a slightly higher classification accuracy, we cannot conclude that the asymmetric network has a better performance for doing the plane detection work. Since only one category of object is included in the experiment, the global vector generated in the symmetric function actually does not make an effort. In further experiments, we can introduce more categories of objects and see how the networks will work on more complicated shapes.</p><p>This experiment shows the potential of neural networks for doing plane detection. According to the experiment results, misclassification is resulting in holes on the detected planes. This is a not a very severe problem for plane detection tasks, as we can apply a 3D Hough transform afterwards, which is robust to missing and contaminated data, to the points of detected planar parts to generate the plane information.</p><p>All the code and data of this experiment can be found over my GitHub repository <a href="https://github.com/IsaacGuan/PointNet-Plane-Detection" target="_blank" rel="external">IsaacGuan/PointNet-Plane-Detection</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Plane detection is a widely used technique that can be applied in many applications, e.g., augmented reality (AR), where we have to detec
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="plane-detection" scheme="https://isaacguan.github.io/tags/plane-detection/"/>
    
      <category term="deep-learning" scheme="https://isaacguan.github.io/tags/deep-learning/"/>
    
      <category term="pointnet" scheme="https://isaacguan.github.io/tags/pointnet/"/>
    
  </entry>
  
  <entry>
    <title>Prepare Your Own Data for PointNet</title>
    <link href="https://isaacguan.github.io/2018/05/04/Prepare-Your-Own-Data-for-PointNet/"/>
    <id>https://isaacguan.github.io/2018/05/04/Prepare-Your-Own-Data-for-PointNet/</id>
    <published>2018-05-05T00:15:58.000Z</published>
    <updated>2018-05-15T02:50:28.901Z</updated>
    
    <content type="html"><![CDATA[<p>Being a novel deep net architecture invariant towards input order, <a href="https://github.com/charlesq34/pointnet" target="_blank" rel="external">PointNet</a> is able to consume unordered point clouds directly and thus has a promising prospect in the field of geometry processing. At present, the most popular implementation of PointNet is based on <a href="https://www.tensorflow.org/" target="_blank" rel="external">TensorFlow</a> and it takes <a href="https://support.hdfgroup.org/HDF5/" target="_blank" rel="external">HDF5</a> as standard input format. It could be a bit confusing for people converting point clouds to HDF5 files and this article is about to tell you how to collect HDF5 datasets for PointNet learning.</p><h1 id="PTS-Data"><a href="#PTS-Data" class="headerlink" title="PTS Data"></a>PTS Data</h1><p>We can download raw data from a certain 3D data repositories, for instance, the <a href="http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html" target="_blank" rel="external">ShapeNetPart</a> dataset. The data directly derived from those repositories is basically in the <a href="http://filext.com/file-extension/PTS" target="_blank" rel="external">PTS</a> file format, which is a set of unordered point coordinates with no headers or trailers. This actually makes things easier, as we can directly read the PTS file line by line and store the point cloud into an array <code>lines</code>. For example, before generating HDF5 datasets, we want that each point cloud has the same length. Thus, a simple subsampler can be applied to the PTS files. The following code snippet shows a random sampler subsampling the point cloud to 2048 points.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">f = open(file, <span class="string">'r+'</span>)</div><div class="line">lines = [line.rstrip() <span class="keyword">for</span> line <span class="keyword">in</span> f]</div><div class="line">slice = random.sample(lines, <span class="number">2048</span>)</div></pre></td></tr></table></figure><h1 id="PLY-Data"><a href="#PLY-Data" class="headerlink" title="PLY Data"></a>PLY Data</h1><p><a href="http://paulbourke.net/dataformats/ply/" target="_blank" rel="external">PLY</a> is a very famous file format that stores 3D data. It has headers to specify the variation and elements of the PLY file. Thus it could be a bit more complicated to deal with such data than PTS data. Luckily, we can find some ready-made tools to read PLY files, e.g., the <a href="https://github.com/dranjan/python-plyfile" target="_blank" rel="external">plyfile</a>, which is able to read the numerical data from the PLY file as a <a href="http://www.numpy.org/" target="_blank" rel="external">NumPy</a> structured array. The installation of this tool is pretty easy, we can get it directly via <a href="https://pypi.org/project/pip/" target="_blank" rel="external">pip</a>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install plyfile</div></pre></td></tr></table></figure><p>For sure, prior to this, we should also have the NumPy installed.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install numpy</div></pre></td></tr></table></figure><p>The deserialization and serialization of PLY file data are done through <code>PlyData</code> and <code>PlyElement</code> instances, so we have to first import them. Besides, the NumPy module also needs to be loaded.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> plyfile <span class="keyword">import</span> PlyData, PlyElement</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure><p>Then we can start to read a PLY file. Concretely, the code is as follows.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plydata = PlyData.read(filename + <span class="string">'.ply'</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, plydata.elements[<span class="number">0</span>].count):</div><div class="line">    data[i] = [plydata[<span class="string">'vertex'</span>][<span class="string">'x'</span>][i], plydata[<span class="string">'vertex'</span>][<span class="string">'y'</span>][i], plydata[<span class="string">'vertex'</span>][<span class="string">'z'</span>][i]]</div></pre></td></tr></table></figure><h1 id="Write-HDF5-File"><a href="#Write-HDF5-File" class="headerlink" title="Write HDF5 File"></a>Write HDF5 File</h1><p>We use the <a href="https://github.com/h5py/h5py" target="_blank" rel="external">h5py</a> package as the interface to the HDF5 data format.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install libhdf5-dev</div><div class="line">pip install h5py</div></pre></td></tr></table></figure><p>We first import this package.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> h5py</div></pre></td></tr></table></figure><p>For creating an HDF5 file, we use the <code>h5py.File()</code> function to initialize it, which takes two arguments. The first argument provides the filename and location, the second the mode. We’re writing the file, so we provide a <code>w</code> for write access.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">f = h5py.File(<span class="string">'data.h5'</span>, <span class="string">'w'</span>)</div></pre></td></tr></table></figure><p>Then we need to define the shape and type of the data to write to the HDF5 file.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data = np.zeros((<span class="number">128</span>, <span class="number">2048</span>, <span class="number">3</span>), dtype = np.uint8)</div></pre></td></tr></table></figure><p>After filling <code>data</code> with the point clouds information read from the PTS or PLY files, we can write it to the HDF5 file <code>f</code>, using the <code>create_dataset</code> function associated with it, where we provide a name for the dataset, and the NumPy array.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">f.create_dataset(<span class="string">'data'</span>, data = data)</div></pre></td></tr></table></figure><p>I have a very concrete example of providing data for PointNet in my GitHub repository <a href="https://github.com/IsaacGuan/PointNet-Plane-Detection" target="_blank" rel="external">IsaacGuan/PointNet-Plane-Detection</a>. We can take the script <a href="https://github.com/IsaacGuan/PointNet-Plane-Detection/blob/master/data/write_hdf5.py" target="_blank" rel="external">here</a> as a reference.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Being a novel deep net architecture invariant towards input order, &lt;a href=&quot;https://github.com/charlesq34/pointnet&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
    
      <category term="data-preprocessing" scheme="https://isaacguan.github.io/tags/data-preprocessing/"/>
    
      <category term="pointnet" scheme="https://isaacguan.github.io/tags/pointnet/"/>
    
  </entry>
  
  <entry>
    <title>RAPter on Ubuntu</title>
    <link href="https://isaacguan.github.io/2018/04/29/RAPter-on-Ubuntu/"/>
    <id>https://isaacguan.github.io/2018/04/29/RAPter-on-Ubuntu/</id>
    <published>2018-04-29T12:12:16.000Z</published>
    <updated>2018-04-29T17:23:06.347Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://github.com/amonszpart/RAPter" target="_blank" rel="external">RAPter</a> gives us a very nice design and implementation for doing man-made scene reconstruction. According to the nature of its algorithm, it also shows a potential use in terms of 3D plane detection. However, the documentation related to it is pretty poor, so I decided to write here a summary of how to install it on <a href="http://releases.ubuntu.com/16.04/" target="_blank" rel="external">Ubuntu 16.04 LTS</a>.</p><h1 id="Workspace"><a href="#Workspace" class="headerlink" title="Workspace"></a>Workspace</h1><p>The first thing is to create a <code>workspace</code> directory in the user’s home directory and a <code>3rdparty</code> subdirectory under <code>workspace</code>. Thus we can define the workspace path <code>WORKSPACE_DIR</code> as <code>$ENV{HOME}/workspace</code> and the third party dependencies path <code>THIRD_PARTY_DIR</code> as <code>${WORKSPACE_DIR}/3rdparty</code>. In the following steps, we will store all the implementations of RAPter tools and experimental data in <code>workspace</code> and download all the dependencies in <code>3rdparty</code>.</p><h1 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h1><p>RAPter requires several dependencies, including <a href="https://opencv.org/" target="_blank" rel="external">OpenCV</a>, <a href="http://pointclouds.org/" target="_blank" rel="external">PointCloudLibrary</a>, <a href="https://projects.coin-or.org/svn/Bonmin/" target="_blank" rel="external">CoinBonmin</a>, and <a href="https://github.com/mkirchner/libfbi" target="_blank" rel="external">libfbi</a>. Prior to downloading and installing all these dependences, we need to first get prepared with a few tools, including <a href="https://git-scm.com/" target="_blank" rel="external">Git</a>, <a href="https://subversion.apache.org/" target="_blank" rel="external">SVN</a>, and <a href="https://subversion.apache.org/" target="_blank" rel="external">CMake</a> via the following commands.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install git</div><div class="line">sudo apt-get install subversion</div><div class="line">sudo apt-get install cmake</div></pre></td></tr></table></figure><h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><p>To install the latest version of OpenCV, we can use the installation script <a href="https://github.com/milq/milq/blob/master/scripts/bash/install-opencv.sh" target="_blank" rel="external">here</a>. We first download this script in the <code>3rdparty</code> directory, and then execute:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bash install-opencv.sh</div></pre></td></tr></table></figure><p>Type the <code>sudo</code> password and OpenCV will be installed.</p><h2 id="PointCloudLibrary"><a href="#PointCloudLibrary" class="headerlink" title="PointCloudLibrary"></a>PointCloudLibrary</h2><p>There is no ready-made script for PointCloudLibrary, so the installation of it could be a bit tedious.</p><h3 id="Setup-Prerequisites"><a href="#Setup-Prerequisites" class="headerlink" title="Setup Prerequisites"></a>Setup Prerequisites</h3><p>The PointCloudLibrary also needs a bunch of prerequisite tools. We use the commands below to have them set up.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install git build-essential linux-libc-dev</div><div class="line">sudo apt-get install cmake cmake-gui </div><div class="line">sudo apt-get install libusb-1.0-0-dev libusb-dev libudev-dev</div><div class="line">sudo apt-get install mpi-default-dev openmpi-bin openmpi-common  </div><div class="line">sudo apt-get install libflann1.8 libflann-dev</div><div class="line">sudo apt-get install libeigen3-dev</div><div class="line">sudo apt-get install libboost-all-dev</div><div class="line">sudo apt-get install libvtk5.10-qt4 libvtk5.10 libvtk5-dev</div><div class="line">sudo apt-get install libqhull* libgtest-dev</div><div class="line">sudo apt-get install freeglut3-dev pkg-config</div><div class="line">sudo apt-get install libxmu-dev libxi-dev </div><div class="line">sudo apt-get install mono-complete</div><div class="line">sudo apt-get install qt-sdk openjdk-8-jdk openjdk-8-jre</div></pre></td></tr></table></figure><h3 id="Build-PointCloudLibrary"><a href="#Build-PointCloudLibrary" class="headerlink" title="Build PointCloudLibrary"></a>Build PointCloudLibrary</h3><p>In the <code>3rdparty</code> directory, PointCloudLibrary is obtained by:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/PointCloudLibrary/pcl.git</div></pre></td></tr></table></figure><p>Now we get into <code>pcl</code>, create a <code>release</code> directory in it, and then follow the <code>cmake</code> build process.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> pcl</div><div class="line">mkdir release</div><div class="line"><span class="built_in">cd</span> release</div><div class="line">cmake -DCMAKE_BUILD_TYPE=None -DCMAKE_INSTALL_PREFIX=/usr \</div><div class="line">      -DBUILD_GPU=ON -DBUILD_apps=ON -DBUILD_examples=ON \</div><div class="line">      -DCMAKE_INSTALL_PREFIX=/usr ..</div><div class="line">make</div></pre></td></tr></table></figure><p>Once the build is finished, we install it by:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">make install</div></pre></td></tr></table></figure><h2 id="CoinBonmin"><a href="#CoinBonmin" class="headerlink" title="CoinBonmin"></a>CoinBonmin</h2><p>As usual, we need to install the prerequisites:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install liblapack-dev</div><div class="line">sudo apt-get install libblas-dev</div><div class="line">sudo apt-get install fortran-compiler</div></pre></td></tr></table></figure><p>Then we download CoinBonmin in <code>3rdparty</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">svn co https://projects.coin-or.org/svn/Bonmin/stable/1.5 CoinBonmin-stable</div></pre></td></tr></table></figure><p>CoinBonmin also requires a third party solver. To get it, we have to first go to the directory <code>CoinBonmin-stable/ThirdParty/Mumps</code> and run:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./get.Mumps</div></pre></td></tr></table></figure><p>To compile and install CoinBonmin, we have to go back to the root directory of it, which is <code>CoinBonmin-stable</code>, and create a <code>build</code> directory in it, then run the <code>cmake</code> process.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">mkdir build</div><div class="line"><span class="built_in">cd</span> build</div><div class="line">../configure -C</div><div class="line">make</div><div class="line">make install</div></pre></td></tr></table></figure><h2 id="libfbi"><a href="#libfbi" class="headerlink" title="libfbi"></a>libfbi</h2><p>We only have to download libfbi in <code>3rdparty</code>, no prerequisites or installation is required.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/mkirchner/libfbi.git</div></pre></td></tr></table></figure><h1 id="Compilation"><a href="#Compilation" class="headerlink" title="Compilation"></a>Compilation</h1><p>Finally, all the dependencies are satisfied. We first get the RAPter in the <code>workspace</code> directory:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/amonszpart/RAPter.git</div></pre></td></tr></table></figure><p>Then get into the root of RAPter, which is <code>RAPter/RAPter</code>, and build it.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">mkdir build</div><div class="line"><span class="built_in">cd</span> build</div><div class="line">cmake -DCMAKE_BUILD_TYPE=Release ..</div><div class="line">make</div></pre></td></tr></table></figure><p>Besides, we may also need to change the default directories of dependencies in <code>CMakelist.txt</code>. For example, following this article, the directory of PointCloudLibrary <code>PCL_DIR</code> should be <code>${THIRD_PARTY_DIR}/pcl/</code> instead of <code>${THIRD_PARTY_DIR}/pcl-trunk2/install/share/pcl-1.8/</code>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The &lt;a href=&quot;https://github.com/amonszpart/RAPter&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;RAPter&lt;/a&gt; gives us a very nice design and implementati
      
    
    </summary>
    
    
      <category term="ubuntu" scheme="https://isaacguan.github.io/tags/ubuntu/"/>
    
      <category term="rapter" scheme="https://isaacguan.github.io/tags/rapter/"/>
    
  </entry>
  
  <entry>
    <title>Tutte Embedding for Parameterization</title>
    <link href="https://isaacguan.github.io/2018/03/19/Tutte-Embedding-for-Parameterization/"/>
    <id>https://isaacguan.github.io/2018/03/19/Tutte-Embedding-for-Parameterization/</id>
    <published>2018-03-19T21:36:33.000Z</published>
    <updated>2018-03-27T01:51:10.496Z</updated>
    
    <content type="html"><![CDATA[<p>In geometry processing, <a href="https://en.wikipedia.org/wiki/Tutte_embedding" target="_blank" rel="external">Tutte embedding</a>, also known as barycentric embedding, can be used for <a href="https://en.wikipedia.org/wiki/Mesh_parameterization" target="_blank" rel="external">mesh parameterization</a> by fixing the boundary vertices of the mesh on a certain convex polygon, and building a <a href="https://en.wikipedia.org/wiki/Tutte_embedding" target="_blank" rel="external">crossing-free straight-line embedding</a> with the interior vertices inside the convex boundary.</p><h1 id="Implementation-of-Tutte-Embedding"><a href="#Implementation-of-Tutte-Embedding" class="headerlink" title="Implementation of Tutte Embedding"></a>Implementation of Tutte Embedding</h1><p>In order to implement a Tutte embedding, we have to use two vectors $\mathbf{u}$ and $\mathbf{v}$ to store the parameterized coordinates, and another two vectors $\bar{\mathbf{u}}$ and $\bar{\mathbf{v}}$ to store the boundary vertices. For the interior vertex $i$, we can directly apply the following equation to it to build the barycentric mapping:$$\begin{equation}\sum_{}a_{i,j}\begin{pmatrix}u_j\\v_j\end{pmatrix}-a_{i,i}\begin{pmatrix}u_i\\v_i\end{pmatrix}=0,\tag{1}\label{eq:1}\end{equation}$$where $j$ denotes the adjacent vertices of $i$.</p><p>For those vertices on the boundary, we use the equation below and set $a_{i,i}=1$, to have them fixed on a convex shape:$$\begin{equation}\sum_{}a_{i,i}\begin{pmatrix}u_i\\v_i\end{pmatrix}=\begin{pmatrix}\bar{u}_i\\\bar{v}_i\end{pmatrix}.\tag{2}\label{eq:2}\end{equation}$$</p><p>Thus, we can build the two linear systems below:$$\begin{equation}\begin{cases}\mathbf{A}\mathbf{u}=\bar{\mathbf{u}},\\\mathbf{A}\mathbf{v}=\bar{\mathbf{v}},\end{cases}\tag{3}\label{eq:3}\end{equation}$$where $\mathbf{A}=(a_{i,j})$ is a sparse matrix storing all the weights $a_{i,j}$. And we can find the parameterized coordinates by solving for the two linear systems.</p><p>I chose a disk as the convex shape for fixing the boundary vertices and used the <a href="https://github.com/GeometryCollective/geometry-processing-js" target="_blank" rel="external">geometry-processing-js</a> library for the data structure of meshes.</p><h1 id="Weighting-Schemes"><a href="#Weighting-Schemes" class="headerlink" title="Weighting Schemes"></a>Weighting Schemes</h1><p>I tried three different weight sets, namely the uniform Laplacian weights, the Laplace-Beltrami weights, and the mean value weights.</p><h2 id="Uniform-Laplacian-Weights"><a href="#Uniform-Laplacian-Weights" class="headerlink" title="Uniform Laplacian Weights"></a>Uniform Laplacian Weights</h2><p>The uniform Laplacian weight set is easiest to build, but it does not take the geometric feature of the mesh into consideration. We can simply set the entries of the weight matrix in the following manner:$$\begin{equation}\begin{cases}a_{i,j}=1,\\a_{i,i}=-\sum_{}a_{i,j}=-D_i,\end{cases}\tag{4}\label{eq:4}\end{equation}$$where $D_i$ is the degree of vertex $i$, i.e., the number of edges connecting $i$.</p><h2 id="Laplace-Beltrami-Weights"><a href="#Laplace-Beltrami-Weights" class="headerlink" title="Laplace-Beltrami Weights"></a>Laplace-Beltrami Weights</h2><p>We can also derive weights from the <a href="https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator" target="_blank" rel="external">Laplace-Beltrami operator</a>:$$\begin{equation}\begin{cases}a_{i,j}=\frac{1}{2A_i}(\cot\alpha_{i,j}+\cot\beta_{i,j}),\\a_{i,i}=-\sum_{}a_{i,j},\end{cases}\tag{5}\label{eq:5}\end{equation}$$where $A_i$ denotes the area of the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" target="_blank" rel="external">Voronoi region</a> of the vertex $i$, $\alpha_{i,j}$ and $\beta_{i,j}$ denote respectively the two corners opposite to the edge $(i,j)$, as shown in Figure 1.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_1.jpg" alt="tutte embedding for parameterization figure 1"></p><center><b>Figure 1:</b> Voronoi region of one vertex.</center><p>For calculating the value of $A_i$, we can use:$$\begin{equation}A_i=\frac{1}{8}\sum_{}(\cot\alpha_{i,j}+\cot\beta_{i,j})\|\mathbf{x}_i-\mathbf{x}_j\|^2,\tag{6}\label{eq:6}\end{equation}$$where $\|\mathbf{x}_i-\mathbf{x}_j\|$ is the length of edge $(i,j)$.</p><p>As far as one triangle from the one-ring neighborhood of a vertex is concerned, which is shown in Figure 2, the Voronoi region inside that triangle can be calculated as:$$\begin{equation}W=\frac{1}{8}\sum_{}(\|\mathbf{x}_i-\mathbf{x}_j\|^2\cot\alpha+\|\mathbf{x}_i-\mathbf{x}_j\|^2\cot\beta).\tag{7}\label{eq:7}\end{equation}$$</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_2.jpg" alt="tutte embedding for parameterization figure 2"></p><center><b>Figure 2:</b> Voronoi region of one triangle of one-ring neighborhood.</center><p>As the data structure of mesh is based on <a href="https://en.wikipedia.org/wiki/Doubly_connected_edge_list" target="_blank" rel="external">half-edge</a>, we can simply find the neighbor of a certain half-edge by retrieving its previous half-edge, e.g., in Figure 2, half-edge $\langle j_2,i\rangle$ is previous to half-edge $\langle i,j_1\rangle$. Thus, by traversing outgoing half-edges associated on a vertex, we can easily calculate the Voronoi region of it using the following code snippet:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> area = <span class="number">0.0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">let</span> h <span class="keyword">of</span> v.adjacentHalfedges()) &#123;</div><div class="line">    <span class="keyword">let</span> u2 = <span class="keyword">this</span>.vector(h.prev).norm2();</div><div class="line">    <span class="keyword">let</span> v2 = <span class="keyword">this</span>.vector(h).norm2();</div><div class="line">    <span class="keyword">let</span> cotAlpha = <span class="keyword">this</span>.cotan(h.prev);</div><div class="line">    <span class="keyword">let</span> cotBeta = <span class="keyword">this</span>.cotan(h);</div><div class="line">    area += (u2 * cotAlpha + v2 * cotBeta) / <span class="number">8</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="Mean-Value-Weights"><a href="#Mean-Value-Weights" class="headerlink" title="Mean Value Weights"></a>Mean Value Weights</h2><p>The Laplace-Beltrami weight takes the geometry of a mesh into account, but it could be negative with obtuse triangles. Based on the mean value theorem, we can derive the mean value weights, which are always positive and generate one-to-one mappings:$$\begin{equation}\begin{cases}a_{i,j}=\frac{1}{\|\mathbf{x}_i-\mathbf{x}_j\|}(\tan\frac{\delta_{i,j}}{2}+\tan\frac{\gamma_{i,j}}{2}),\\a_{i,i}=-\sum_{}a_{i,j},\end{cases}\tag{8}\label{eq:8}\end{equation}$$where $\|\mathbf{x}_i-\mathbf{x}_j\|$ is the length of edge $(i,j)$, $\delta_{i,j}$ and $\gamma_{i,j}$ are two neighboring corners on both sides of edge $(i,j)$, as shown in Figure 3.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_3.jpg" alt="tutte embedding for parameterization figure 3"></p><center><b>Figure 3</b></center><p>In the real implementation, like what we did in calculating the Voronoi region, we can still take advantage of the sequence of half-edges to find the corresponding corners:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> delta = h.next.corner;</div><div class="line"><span class="keyword">let</span> gamma = h.twin.prev.corner;</div></pre></td></tr></table></figure><h1 id="Results-and-Conclusion"><a href="#Results-and-Conclusion" class="headerlink" title="Results and Conclusion"></a>Results and Conclusion</h1><p>The results of Tutte embedding are shown in Figure 4, applied to different objects with different weight sets.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_4.jpg" alt="tutte embedding for parameterization figure 4"></p><center><b>Figure 4:</b> Results of Tutte embedding.</center><p>We can conclude from the results that the Laplace-Beltrami weight set and mean value weight set can better preserve the shape of triangles in the original mesh. This leads to the interior holes of a mesh to enlarge, which is clearly shown in the beetle. For better illustrating this property, we can take a close look at the mesh (Figure 5).</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_5.jpg" alt="tutte embedding for parameterization figure 5"></p><center><b>Figure 5:</b> Parts of parameterizations of cow.</center><p>Figure 5 takes the same 200×200 pixels parts from the screenshots of the parametrizations of the cow object. It shows that the uniform Laplacian weight can cause the triangles on the parameterization to become more regular, but the triangles on the parameterized mesh of the Laplace-Beltrami weight and the mean value weight are more similar to those on the original mesh.</p><p>Besides, the Laplace-Beltrami weight and the mean value weight can even better preserve the patterns on the mesh. As shown in Figure 6, the features on the face object (i.e., the mouth and the eyes) are more distinguishable if we use the Laplace-Beltrami weight or the mean value weight.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_6.jpg" alt="tutte embedding for parameterization figure 6"></p><center><b>Figure 6:</b> Parameterizations of face.</center><p>I have also uploaded the project to this blog: <a href="/projects/tutte-embedding">/projects/tutte-embedding</a>, we can go there to see how the algorithms work.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In geometry processing, &lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte_embedding&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tutte embedding&lt;/a&gt;, also 
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="parameterization" scheme="https://isaacguan.github.io/tags/parameterization/"/>
    
      <category term="tutte-embedding" scheme="https://isaacguan.github.io/tags/tutte-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Hough Transform for Plane Detection</title>
    <link href="https://isaacguan.github.io/2018/02/28/Hough-Transform-for-Plane-Detection/"/>
    <id>https://isaacguan.github.io/2018/02/28/Hough-Transform-for-Plane-Detection/</id>
    <published>2018-03-01T02:02:15.000Z</published>
    <updated>2018-03-20T00:16:37.654Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Hough_transform" target="_blank" rel="external">Hough transform</a> is a method for detecting parameterized objects, typically used for lines and circles in 2D space. Nowadays, with the proliferation of acquisitive devices, deriving a massive point cloud is an easy task. As we can also parameterize objects in 3D, Hough transform can be applied to detect planes in 3D point clouds as well.</p><h1 id="Parameterization-of-a-Plane"><a href="#Parameterization-of-a-Plane" class="headerlink" title="Parameterization of a Plane"></a>Parameterization of a Plane</h1><p>Similar to a line in 2D space, a plane in 3D can be described using a <a href="https://en.wikipedia.org/wiki/Linear_equation#Slope%E2%80%93intercept_form" target="_blank" rel="external">slope–intercept equation</a>, where $k_x$ is the slope in x-direction, $k_y$ is the slope in y-direction, and $b$ is the intercept on z-axis:$$\begin{equation}z=k_xx+k_yy+b.\tag{1}\label{eq:1}\end{equation}$$With $\eqref{eq:1}$ we can simply parameterize a plane as $(k_x,k_y,b)$.</p><p>However, values of $k_x$, $k_y$, and $b$ are unbounded, which would pose a problem when we try to parameterize a vertical plane. Thus, for computational reasons, we can write the plane equation in the <a href="https://en.wikipedia.org/wiki/Hesse_normal_form" target="_blank" rel="external">Hesse normal form</a>, where $\theta$ is the angle between the normal vector of this plane and the x-axis, $\phi$ is the angle between the normal vector and z-axis, and $\rho$ is the distance from the plane to the origin:$$\begin{equation}x\cos\theta\sin\phi+y\sin\phi\sin\theta+z\cos\phi=\rho.\tag{2}\label{eq:2}\end{equation}$$As shown in Figure 1, the plane can be parameterized as $(\theta,\phi,\rho)$.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_1.jpg" alt="hough transform for plane detection figure 1"></p><center><b>Figure 1:</b> Parameterization of a plane.</center><p>To find planes in a 3D point cloud, we have to calculate the Hough transform for each point, which is to say that we parameterize every possible plane that goes through every point in the $(\theta,\phi,\rho)$ Hough space. For instance, Figure 2 shows the parameterization of three points $(0,0,1)$, $(0,1,0)$, and $(1,0,0)$. Each point is marked as a 3D sinusoid curve in Hough space and the intersection which is marked in black denotes the plane defined by the three points.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_2.jpg" alt="hough transform for plane detection figure 2"></p><center><b>Figure 2:</b> Transformation of three points from original space to Hough space.</center><h1 id="Hough-Methods"><a href="#Hough-Methods" class="headerlink" title="Hough Methods"></a>Hough Methods</h1><p>Basically, the algorithm for Hough transform can be described as a voting method, where we discretize the Hough space with a bunch of $(\theta,\phi,\rho)$ cells. A data structure called accumulator then is needed to store all these cells with a scoring parameter for every cell. Incrementing a cell means increasing the score of it by +1. Each point votes for all cells of $(\theta,\phi,\rho)$ that define a plane on which it may lie.</p><h2 id="Standard-Hough-Transform"><a href="#Standard-Hough-Transform" class="headerlink" title="Standard Hough Transform"></a>Standard Hough Transform</h2><p>A most basic and naïve Hough transform algorithm for plane detection is outlined as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Traverse all the points in the point cloud $P$;</p><p><em>Step 2:</em> For each point $\mathbf{p}_i$ in $P$, vote for all the $A(\theta,\phi,\rho)$ cells in the accumulator $A$ defined by this point;</p><p><em>Step 3:</em> After the whole iteration, search for the most prominent cells in the accumulator $A$, that define the detected planes in $P$.</p><p><strong><em>End</em></strong></p><p>The standard Hough transform is performed in two stages: incrementing the cells, which needs $O(|P|\cdot N_\phi\cdot N_\theta)$ operations, and searching for the most prominent cells, which takes $O(N_\rho\cdot N_\phi\cdot N_\theta)$ time, where $|P|$ is the size of the point cloud, $N_\phi$ is the number of cells in direction of $\phi$, $N_\theta$ in direction of $\theta$, and $N_\rho$ in direction of $\rho$.</p><h2 id="Probabilistic-Hough-Transform"><a href="#Probabilistic-Hough-Transform" class="headerlink" title="Probabilistic Hough Transform"></a>Probabilistic Hough Transform</h2><p>In the standard Hough transform, the size of the point cloud $|P|$ is usually much larger than the number $N_\rho \cdot N_\phi \cdot N_\theta$ of cells in the accumulator array. We can simply reduce the number of points to improve the computational expense. Thus, the standard Hough transform can be adapted to the probabilistic Hough transform, which is shown as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Determine the size value $m$ and the threshold value $t$;</p><p><em>Step 2:</em> randomly select $m$ points to create $P^m\subset P$;</p><p><em>Step 3:</em> Do the standard Hough transform on the point set $P^m$;</p><p><em>Step 4:</em> Delete the cells from the accumulator $A$ with a value that does not reach $t$, and search for the most prominent cells in $A$, that define the detected planes in $P$.</p><p><strong><em>End</em></strong></p><p>The $m$ points (with $m&lt;|P|$) are randomly selected from the point cloud $P$, so the dominant part of the runtime is proportional to $O(m\cdot N_\phi\cdot N_\theta)$. However, the optimal choice of $m$ and the threshold $t$ depends on the actual problem, e.g., the number of planes, or the noise in the point cloud.</p><h3 id="Adaptive-Probabilistic-Hough-Transform"><a href="#Adaptive-Probabilistic-Hough-Transform" class="headerlink" title="Adaptive Probabilistic Hough Transform"></a>Adaptive Probabilistic Hough Transform</h3><p>In order to find the optimal subsample of the point cloud, we can use an adaptive method to determine the reasonable number of selected points. The adaptive probabilistic Hough transform monitors the accumulator. The structure of the accumulator changes dynamically during the voting phase. As soon as stable structures emerge and turn into significant peaks, voting is terminated.</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Check the stability order $m_k$ of the list of $k$ peaks $S_k$ in the accumulator, if it reaches the threshold value $t_k$ then finish;</p><p><em>Step 2:</em> Randomly select a small subset $P^m\subset P$;</p><p><em>Step 3:</em> Do the standard Hough transform on the point set $P^m$;</p><p><em>Step 4:</em> Merge the active list of peaks $S_k$ with the previous one, determine the stability order $m_k$, goto <em>Step 1</em>;</p><p><strong><em>End</em></strong></p><p>In this algorithm, The stability is described by a set $S_k$ of $k$ peaks in the list, if the set contains all largest peaks before and after one update phase. The number $m_k$ of consecutive lists in which $S_k$ is stable is called the stability order of $S_k$.</p><h3 id="Progressive-Probabilistic-Hough-Transform"><a href="#Progressive-Probabilistic-Hough-Transform" class="headerlink" title="Progressive Probabilistic Hough Transform"></a>Progressive Probabilistic Hough Transform</h3><p>The progressive probabilistic Hough transform calculates stopping time for random selection of points. The algorithm stops whenever a cell count exceeds a threshold.</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Check the input point cloud $P$, if it is empty then finish;</p><p><em>Step 2:</em> Update the accumulator with a single point $\mathbf{p}_i$ randomly selected from $P$;</p><p><em>Step 3:</em> Remove $\mathbf{p}_i$ from $P$ and add it to $P_\text{voted}$;</p><p><em>Step 4:</em> Check if the highest peak in the accumulator that was modified by the new point is higher than the threshold $t$, if not then goto <em>Step 1</em>;</p><p><em>Step 5:</em> Select all points from $P$ and $P_\text{voted}$ that are close to the plane defined by the highest peak and add them to $P_\text{plane}$;</p><p><em>Step 6:</em> Search for the largest connected region $P_\text{region}$ in $P_\text{plane}$ and remove from $P$ all points in $P_\text{region}$;</p><p><em>Step 7:</em> Reset the accumulator by unvoting all the points in $P_\text{region}$;</p><p><em>Step 8:</em> If the area covered by $P_\text{region}$ is larger than a threshold, add it to the output list, goto <em>Step 1</em>;</p><p><strong><em>End</em></strong></p><p>In this algorithm, $P_\text{voted}$ is the point set of all the voted points before a plane is detected, $P_\text{plane}$ is the set of points in the detected planes, and $P_\text{region}$ denotes the largest connected region in $P_\text{plane}$. For determining the stopping time, the threshold $t$ is predicated on the percentage of votes for one cell from all points that have voted.</p><h2 id="Randomized-Hough-Transform"><a href="#Randomized-Hough-Transform" class="headerlink" title="Randomized Hough Transform"></a>Randomized Hough Transform</h2><p>As we know the fact that a plane is defined by three points. For detecting planes, three points from the input space are mapped onto one point in the Hough space. When a plane is represented by a large number of points, it is more likely that three points from this plane are randomly selected. Eventually, the cells corresponding to actual planes receive more votes and are distinguishable from the other cells. Inspired by this idea, we can come up with an algorithm described as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Check the input point cloud $P$, if it is empty then finish;</p><p><em>Step 2:</em> Randomly pick three points $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$ from $P$;</p><p><em>Step 3:</em> If $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$ fulfill the distance criterion, calculate plane $(\theta,\phi,\rho)$ spanned by $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$, and increment cell $A(\theta,\phi,\rho)$ in the accumulator space;</p><p><em>Step 4:</em> If the count of $|A(\theta,\phi,\rho)|$ reaches the threshold $t$, $(\theta,\phi,\rho)$ parameterize the detected plane, delete all points close to $(\theta,\phi,\rho)$ from $P$, and reset the accumulator $A$;</p><p><em>Step 5:</em> Goto Step 1;</p><p><strong><em>End</em></strong></p><p>This algorithm simply decreases the number of cells touched by exploiting the fact that a curve with $n$ parameters is defined by $n$ points. And also note that, if points are very far apart, they most likely do not belong to one plane. To take care of this and to diminish errors from sensor noise a distance criterion is introduced: $\mathrm{dist}(\mathbf{p}_1,\mathbf{p}_2,\mathbf{p}_3)\leq\mathrm{dist}_\text{max}$, i.e., the maximum point-to-point distance between $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$ is below a fixed threshold.</p><h1 id="Accumulator"><a href="#Accumulator" class="headerlink" title="Accumulator"></a>Accumulator</h1><p>An inappropriate accumulator may lead to detection failures of some specific planes and difficulties in finding local maxima, displays low accuracy, large storage space, and low speed. A trade-off has to be found between a coarse discretization that accurately detects planes and a small number of cells in the accumulator to decrease the time needed for the Hough transform.</p><h2 id="Accumulator-Array"><a href="#Accumulator-Array" class="headerlink" title="Accumulator Array"></a>Accumulator Array</h2><p>For the standard implementation of the 2D Hough transform, the Hough space is divided into $N_\rho\times N_\theta$ rectangular cells. The size of the cells is variable and is chosen problem-dependent. Using the same subdivision for the 3D Hough space by dividing it into cuboid cells results in the patches seen in Figure 3. The cells closer to the poles are smaller and comprise less normal vectors. This means voting favors the larger equatorial cells.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_3.jpg" alt="hough transform for plane detection figure 3"></p><center><b>Figure 3:</b> Accumulator array.</center><h2 id="Accumulator-Cube"><a href="#Accumulator-Cube" class="headerlink" title="Accumulator Cube"></a>Accumulator Cube</h2><p>We can also project the unit sphere $S^2$ onto the smallest cube that contains the sphere using the diffeomorphism:$$\begin{equation}\phi:S^2\to\mathrm{cube},s\mapsto\|s\|_\infty.\tag{3}\label{eq:3}\end{equation}$$</p><p>Each face of the cube is divided into a regular grid, which is shown in Figure 4. With this design of the accumulator, the difference of size between the patches on the unit sphere is negligible.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_4.jpg" alt="hough transform for plane detection figure 4"></p><center><b>Figure 4:</b> Accumulator cube.</center><h2 id="Accumulator-Ball"><a href="#Accumulator-Ball" class="headerlink" title="Accumulator Ball"></a>Accumulator Ball</h2><p>The commonly used design, the accumulator array, which is shown in Figure 3, causes the irregularity between the patches on the unit sphere. To solve this issue, we can simply customize the resolution in polar coordinates depending on the position of the sphere. The resolution of the longitude $\phi$ is kept as for the accumulator array, which is defined as $\phi^\prime$. In the $\theta$ direction, the largest latitude circle is the equator located at $\phi=0$. For the unit sphere it has the $l_\text{max}=2\pi$. The length of the latitude circle in the middle of the segment located above $\phi_i$ is given by $l_i=2\pi(\phi+\phi^\prime)$. The step width in $\theta$ direction for each slice is now computed as $\theta_{\phi_i}=\frac{360^\circ\cdot l_\text{max}}{l_i\cdot N_\theta}$, where $N_\theta$ is a constant that can be customized. The resulting design is illustrated in Figure 5.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_5.jpg" alt="hough transform for plane detection figure 5"></p><center><b>Figure 5:</b> Accumulator ball.</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Hough_transform&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hough transform&lt;/a&gt; is a method for detecting 
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="hough-transform" scheme="https://isaacguan.github.io/tags/hough-transform/"/>
    
      <category term="parameterization" scheme="https://isaacguan.github.io/tags/parameterization/"/>
    
      <category term="plane-detection" scheme="https://isaacguan.github.io/tags/plane-detection/"/>
    
  </entry>
  
  <entry>
    <title>Mesh Smoothing</title>
    <link href="https://isaacguan.github.io/2018/01/26/Mesh-Smoothing/"/>
    <id>https://isaacguan.github.io/2018/01/26/Mesh-Smoothing/</id>
    <published>2018-01-26T17:45:19.000Z</published>
    <updated>2018-02-06T18:34:11.573Z</updated>
    
    <content type="html"><![CDATA[<p>Mesh smoothing, also known as mesh denoising, is an important and widely discussed topic in geometry processing. Basically, a mesh smoothing method takes three steps: loading a mesh from a file; smoothing that mesh; outputting the mesh to a file. Recently, I implemented a few basic algorithms regarding mesh smoothing.</p><h1 id="Selection-of-Mesh-Library"><a href="#Selection-of-Mesh-Library" class="headerlink" title="Selection of Mesh Library"></a>Selection of Mesh Library</h1><p>There are a lot of mesh libraries that give us a data structure such that we can use to build a mesh for a 3D object. What I am using is the <a href="https://github.com/GeometryCollective/geometry-processing-js" target="_blank" rel="external">geometry-processing-js</a> library. It is a library written in JavaScript which constructs a mesh based on <a href="https://en.wikipedia.org/wiki/Doubly_connected_edge_list" target="_blank" rel="external">half-edge</a>. It also gives plenty of predefined functions regarding the components of a mesh e.g. half-edge, vertex, edge, face, etc. Besides, it also includes a linear algebra library which makes it handy for users to do matrix-vector operations.</p><h1 id="Mesh-Smoothing-Algorithms"><a href="#Mesh-Smoothing-Algorithms" class="headerlink" title="Mesh Smoothing Algorithms"></a>Mesh Smoothing Algorithms</h1><p>I implemented two algorithms for mesh smoothing, a mean filtering algorithm, and a weighted mean filtering algorithm.</p><h2 id="Mean-Filtering-Algorithm"><a href="#Mean-Filtering-Algorithm" class="headerlink" title="Mean Filtering Algorithm"></a>Mean Filtering Algorithm</h2><p>A most naïve mean filtering algorithm goes as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Create a hash map $\mathrm{positions}$ mapping each vertex with its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, calculate $\mathrm{positions}[\mathbf{v}]$ by the average position of the neighbor vertices in the one-ring neighborhood of $\mathbf{v}$;</p><p><em>Step 3:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><p>However, if the object has some holes in it, this algorithm could cause the holes to enlarge after times of iteration, which is shown in Figure 1.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_1.jpg" alt="mesh smoothing figure 1"></p><center><b>Figure 1:</b> Holes enlarge on the object, steps = 5.</center><p>Sometimes, we don’t like such distortion, so it is necessary to preserve the boundary of holes on the mesh. We can add a step to the algorithm to constrain the vertices of boundary edges:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Create a hash map $\mathrm{positions}$ mapping each vertex with its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, calculate $\mathrm{positions}[\mathbf{v}]$ by the average position of the neighbor vertices in the one-ring neighborhood of $\mathbf{v}$;</p><p><em>Step 3:</em> To preserve the boundary of holes on the mesh, if vertex $\mathbf{v}$ is on the boundary, update $\mathrm{positions}[\mathbf{v}]$ to the original position of vertex $\mathbf{v}$ in the mesh;</p><p><em>Step 4:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><h2 id="Weighted-Mean-Filtering-Algorithm"><a href="#Weighted-Mean-Filtering-Algorithm" class="headerlink" title="Weighted Mean Filtering Algorithm"></a>Weighted Mean Filtering Algorithm</h2><p>The weighting method simply weights each neighboring vertex by the area of the two faces incident to it. The weighted mean filtering algorithm goes like this:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em>  Create a hash map $\mathrm{positions}$ that maps each vertex to its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, traverse its neighborhood vertices. And for each neighbor vertex $\mathbf{v}^\ast$ calculate the weight value $w^\ast$ on each neighborhood vertex by adding the area of the two incident faces together;</p><p><em>Step 3:</em> After the traversal of neighborhood vertices, calculate the total weight $w$ on the vertex $\mathbf{v}$ by accumulating every $w^\ast$, and calculate $\mathrm{positions}[\mathbf{v}]=\frac{\sum_{}w^\ast\cdot\mathbf{v}^\ast}{w}$;</p><p><em>Step 4:</em> To preserve the boundary of holes on the mesh, if vertex $\mathbf{v}$ is on the boundary, update $\mathrm{positions}[\mathbf{v}]$ to the original position of vertex $\mathbf{v}$ in the mesh;</p><p><em>Step 5:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><p>However, in the implementation, I found this could cause the faces of the mesh overlapping one another after several times of iterations, as the position of vertex could fall outside its one-ring neighborhood. As shown in Figure 2, overlapping occurs after 5 times of iteration.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_2.jpg" alt="mesh smoothing figure 2"></p><center><b>Figure 2:</b> Overlapping of faces, steps = 5.</center><p>In order to prevent such thing from happening, I tried to find those vertices which could fall outside its neighborhood during an iteration. The one-ring neighborhood of one vertex is actually a star-shaped polygon so that we can use the left-turn/right-turn check to decide whether the vertex is inside it or not.</p><p>To find out the star-shaped neighborhood of a certain vertex $\mathbf{v}$, we should first project all the neighbors onto the plane which is defined by vertex $\mathbf{v}$ and its normal vector $\mathbf{n}$. We can find the projection $\mathbf{q}=(x^\prime,y^\prime,z^\prime)$ of vertex $\mathbf{p}=(x,y,z)$ on the plane that defined by the normal $\mathbf{n}=(a,b,c)$ and vertex $\mathbf{v}=(x_0,y_0,z_0)$ in the way below.</p><p>There are following implicit geometric relationships between $\mathbf{p}$, $\mathbf{q}$, $\mathbf{n}$, and $\mathbf{v}$: $(\mathbf{q}-\mathbf{p})\parallel\mathbf{n}$ and $(\mathbf{q}-\mathbf{v})\perp\mathbf{n}$.</p><p>The following equation can be derived from $(\mathbf{q}-\mathbf{p})\parallel\mathbf{n}$:$$\begin{equation}\frac{x^\prime-x}{a}=\frac{y^\prime-y}{b}=\frac{z^\prime-z}{c}=t.\tag{1}\label{eq:1}\end{equation}$$</p><p>And from $(\mathbf{q}-\mathbf{v})\perp\mathbf{n}$, we can derive:$$\begin{equation}a(x^\prime-x_0)+b(y^\prime-y_0)+c(z^\prime-z_0)=0.\tag{2}\label{eq:2}\end{equation}$$</p><p>Combining $\eqref{eq:1}$ and $\eqref{eq:2}$, we can solve for $t$:$$\begin{equation}t=\frac{ax_0+by_0+cz_0-(ax+by+cz)}{a^2+b^2+c^2}.\tag{3}\label{eq:3}\end{equation}$$</p><p>Then we can solve for the projection $\mathbf{q}=(x^\prime,y^\prime,z^\prime)$ by combining $\eqref{eq:1}$ and $\eqref{eq:3}$.</p><p>As the data structure of mesh is based on half-edge, and all the faces are defined by half-edges in counterclockwise order. Thus, for each vertex $\mathbf{v}$, we can simply get the boundary edges of a star-shaped neighborhood in counterclockwise order, project them along with the position to be updated $\mathrm{positions}[\mathbf{v}]$ onto the plane that is defined by vertex $\mathbf{v}$ and its normal vector $\mathbf{n}$. Then we can do the left-turn/right-turn check on that plane to see whether the new position for vertex $\mathbf{v}$ $\mathrm{positions}[\mathbf{v}]$ is outside its neighborhood polygon or not.</p><p>So the modified algorithm goes as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Create a hash map $\mathrm{positions}$ that maps each vertex to its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, traverse its neighborhood vertices. And for each neighbor vertex $\mathbf{v}^\ast$ calculate the weight value $w^\ast$ on each neighborhood vertex by adding the area of the two incident faces together;</p><p><em>Step 3:</em> After the traversal of neighborhood vertices, calculate the total weight $w$ on the vertex $\mathbf{v}$ by accumulating every $w^\ast$, and calculate $\mathrm{positions}[\mathbf{v}]=\frac{\sum_{}w^\ast\cdot\mathbf{v}^\ast}{w}$;</p><p><em>Step 4:</em> Check whether $\mathrm{positions}[\mathbf{v}]$ falls outside its one-ring neighborhood. If so, use the mean filtering algorithm to update the new position;</p><p><em>Step 5:</em> To preserve the boundary of holes on the mesh, if vertex $\mathbf{v}$ is on the boundary, update $\mathrm{positions}[\mathbf{v}]$ to the original position of vertex $\mathbf{v}$ in the mesh;</p><p><em>Step 6:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><h1 id="Results-and-Conclusion"><a href="#Results-and-Conclusion" class="headerlink" title="Results and Conclusion"></a>Results and Conclusion</h1><p>As for the mean filtering algorithm, Figure 3 shows the results after applying it to different objects with different step numbers.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_3.jpg" alt="mesh smoothing figure 3"></p><center><b>Figure 3:</b> Results of mean filtering algorithm.</center><p>As for the weighted mean filtering algorithm, Figure 4 shows the results after applying it to different objects with different step numbers.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_4.jpg" alt="mesh smoothing figure 4"></p><center><b>Figure 4:</b> Results of weighted mean filtering algorithm.</center><p>We can conclude that the results of both algorithms basically look alike, and both of them could cause the volume of the object to shrink if we apply too many times.</p><p>I have also uploaded the project to this blog: <a href="/projects/mesh-smoothing">/projects/mesh-smoothing</a>, we can go there to see how exactly the algorithms work.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Mesh smoothing, also known as mesh denoising, is an important and widely discussed topic in geometry processing. Basically, a mesh smooth
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="mesh-smoothing" scheme="https://isaacguan.github.io/tags/mesh-smoothing/"/>
    
  </entry>
  
  <entry>
    <title>Implementation of Voronoi Diagram and Delaunay Triangulation</title>
    <link href="https://isaacguan.github.io/2017/12/22/Implementation-of-Voronoi-Diagram-and-Delaunay-Triangulation/"/>
    <id>https://isaacguan.github.io/2017/12/22/Implementation-of-Voronoi-Diagram-and-Delaunay-Triangulation/</id>
    <published>2017-12-23T01:28:50.000Z</published>
    <updated>2017-12-24T20:09:25.280Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" target="_blank" rel="external">Voronoi diagram</a> and the <a href="https://en.wikipedia.org/wiki/Delaunay_triangulation" target="_blank" rel="external">Delaunay triangulation</a> are dual representations of a set of points to each other. Due to their wide applications in science and technology, the Voronoi diagram and the Delaunay triangulation play important roles in the field of computational geometry. In this post, I am going to introduce an implementation of an algorithm to derive both the Voronoi diagram and the Delaunay triangulation of a set of points in the plane.</p><h1 id="Voronoi-Diagram-and-Delaunay-Triangulation"><a href="#Voronoi-Diagram-and-Delaunay-Triangulation" class="headerlink" title="Voronoi Diagram and Delaunay Triangulation"></a>Voronoi Diagram and Delaunay Triangulation</h1><p>The Voronoi diagram of a set of points, also known as Thiessen polygons, is a partitioning of a plane into regions by a set of continuous polygons consisting of perpendicular bisectors of the connecting lines of two adjacent points. These regions are called Voronoi cells. And for each point in the set, there is a corresponding Voronoi cell consists of all points closer to that point than to any other.</p><p>The Delaunay triangulation of a set of points is dual to its Voronoi diagram. It is a collection of connected but non-overlapping triangles, and the outer circumcircle of these triangles does not contain any other points in this set.</p><h1 id="Design-of-the-Algorithm"><a href="#Design-of-the-Algorithm" class="headerlink" title="Design of the Algorithm"></a>Design of the Algorithm</h1><p>There are a lot of ways to generate a Voronoi diagram from a set of points in the plane. In this implementation, I obtained the Voronoi diagram from generating its dual, the Delaunay triangulation. Generally speaking, for the set of $n$ points $P=\{\mathbf{p}_1,\mathbf{p}_2,\ldots,\mathbf{p}_n\}$ in $\mathbb{R}^2$, the algorithm goes in this way: the Delaunay triangulation of the set of points is firstly generated, then we calculate the center of the circumcircle of each triangle, and finally, we connect these centers with straight lines and form the polygon mesh generated from the vertices of the triangles.</p><h2 id="Design-of-the-Data-Structure"><a href="#Design-of-the-Data-Structure" class="headerlink" title="Design of the Data Structure"></a>Design of the Data Structure</h2><p>I implemented this algorithm in the object-oriented language, so the design of the data structure is in the form of classes.</p><p>Point:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">Point</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">double</span> x, y, z;</div><div class="line">    <span class="keyword">public</span> List&lt;<span class="keyword">int</span>&gt; adjoinTriangles;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Point</span>(<span class="params"><span class="keyword">double</span> x, <span class="keyword">double</span> y, <span class="keyword">double</span> z</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.x = x;</div><div class="line">        <span class="keyword">this</span>.y = y;</div><div class="line">        <span class="keyword">this</span>.z = z;</div><div class="line">        adjoinTriangles = <span class="keyword">new</span> List&lt;<span class="keyword">int</span>&gt;();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Voronoi edge:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">VoronoiEdge</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> Point start, end;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">VoronoiEdge</span>(<span class="params">Point start, Point end</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.start = start;</div><div class="line">        <span class="keyword">this</span>.end = end;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Delaunay edge:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">DelaunayEdge</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">int</span> start, end;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DelaunayEdge</span>(<span class="params"><span class="keyword">int</span> start, <span class="keyword">int</span> end</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.start = start;</div><div class="line">        <span class="keyword">this</span>.end = end;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Triangle:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">Triangle</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">int</span> vertex1, vertex2, vertex3;</div><div class="line">    <span class="keyword">public</span> Point center;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">double</span> radius;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Triangle</span>(<span class="params"><span class="keyword">int</span> vertex1, <span class="keyword">int</span> vertex2, <span class="keyword">int</span> vertex3</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.vertex1 = vertex1;</div><div class="line">        <span class="keyword">this</span>.vertex2 = vertex2;</div><div class="line">        <span class="keyword">this</span>.vertex3 = vertex3;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>On top of that, I defined a point list and a triangle list under the Collections class to store all the points in the point set $P$ and all the triangles during the procedure of triangulation as global variables:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">Collections</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;Point&gt; allPoints;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;Triangle&gt; allTriangles;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Thus, in the <code>Point</code> class, <code>DelaunayEdge</code> class, and <code>Triangle</code> class, I can use an integer to represent the index of a certain point or triangle from the global lists and retrieve it directly.</p><h2 id="Build-up-the-Vonoroi-Diagram"><a href="#Build-up-the-Vonoroi-Diagram" class="headerlink" title="Build up the Vonoroi Diagram"></a>Build up the Vonoroi Diagram</h2><p>The algorithm of building up a Vonoroi diagram goes in this way:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Obtain the Delaunay triangulation by generating the list of Delaunay edges.</p><p><em>Step 2:</em> Traverse all the Delaunay edges.</p><p><em>Step 3:</em> For each Delaunay edge, traverse the two triangle lists stored with the starting point and the endpoint, and find the two same triangles in the two lists which are the adjacent triangles of this Delaunay edge.</p><p><em>Step 4:</em> Construct a Voronoi edge by connecting the two centers of the circumcircles of the two adjacent triangles and add it to the Voronoi edge list.</p><p><strong><em>End</em></strong></p><h2 id="Conduct-the-Delaunay-Triangulation"><a href="#Conduct-the-Delaunay-Triangulation" class="headerlink" title="Conduct the Delaunay Triangulation"></a>Conduct the Delaunay Triangulation</h2><p>I planned to apply the <a href="https://en.wikipedia.org/wiki/Bowyer%E2%80%93Watson_algorithm" target="_blank" rel="external">Bowyer–Watson algorithm</a> for computing the Delaunay triangulation. A most naïve Bowyer–Watson algorithm goes like this:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Construct a “super” triangle that covers all the points from the point set, add it to the Delaunay triangle list. </p><p><em>Step 2:</em> Insert points from the point set $P=\{\mathbf{p}_1,\mathbf{p}_2,\ldots,\mathbf{p}_n\}$ to the “super” triangle one by one.</p><p><em>Step 3:</em> For each point $\mathbf{p}_i$ inserted, traverse the Delaunay triangle list to find all the triangles whose circumcircles cover this point $\mathbf{p}_i$ as invalid triangles, delete these triangles from the Delaunay triangle list and delete all the common edges of these triangles, and leave a star-shaped polygonal hole.</p><p><em>Step 4:</em> Connect the point $\mathbf{p}_i$ to all the vertices of this star-shaped polygon, and add the newly formed triangles to the Delaunay triangle list.</p><p><em>Step 5:</em> After all the points are inserted, obtain the Delaunay edge list from the Delaunay triangle list, and delete the edges from the Delaunay edge list that contains a vertex of the original “super” triangle.</p><p><strong><em>End</em></strong></p><p>The following pictures can better illustrate the key steps of this algorithm. As shown in Figure 1, when a new point is inserted, all the triangles whose circumcircles contain this point will be found. The common edges of these triangles, which are highlighted in yellow, will be deleted, leaving the star-shaped boundary in red.</p><p><img src="/img/implementation_of_voronoi_diagram_and_delaunay_triangulation/implementation_of_voronoi_diagram_and_delaunay_triangulation_1.jpg" alt="implementation of voronoi diagram and delaunay triangulation figure 1"></p><center><b>Figure 1:</b> A new point inserted.</center><p>And then, the inserted point will be connected to all the vertices of the star-shaped polygon, as shown in Figure 2, the new Delaunay triangles will be formed.</p><p><img src="/img/implementation_of_voronoi_diagram_and_delaunay_triangulation/implementation_of_voronoi_diagram_and_delaunay_triangulation_2.jpg" alt="implementation of voronoi diagram and delaunay triangulation figure 2"></p><center><b>Figure 2:</b> Connecting to the inserted point.</center><p>However, the aforementioned naïve manner of Delaunay triangulation is clearly an $O(n^2)$ time algorithm that is incapable of handling a massive amount of points.</p><p>For improving its efficiency, we can first sort the set of points by x-coordinates and then use an open list and a closed list to store all the Delaunay triangles. In each time a point is inserted, all the triangles with circumcircles to the left of the inserting point are put into the closed list and removed from the open list. All the new triangles generated in an insertion are put into the open list. So in each time of insertion, we just have to traverse the open list to find the invalid triangles, the length of the sequential search of triangles is much reduced.</p><p>Besides, in order to derive the Voronoi diagram, when a new point is inserted, all the newly formed triangles that are incident on the point are put into the triangle list that is stored with the point.</p><p>Thus, the optimized algorithm goes as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Sort the points in the point set $P=\{\mathbf{p}_1,\mathbf{p}_2,\ldots,\mathbf{p}_n\}$ by x-coordinate.</p><p><em>Step 2:</em> Construct a “super” triangle that covers all the points from the point set, add it to the open list. </p><p><em>Step 3:</em> Insert points from $P$ to the “super” triangle one by one in ascending order of x-coordinates.</p><p><em>Step 4:</em> For each point $\mathbf{p}_i$ inserted, traverse the open list to: 1) find all the triangles with circumcircles lying to the left of the point $\mathbf{p}_i$, delete these triangles from the open list and add them to the closed list; 2) find all the triangles with circumcircles covering this point $\mathbf{p}_i$ as invalid triangles, delete these triangles from the open list and the triangle list stored with $\mathbf{p}_i$, delete all the common edges of these triangles, leaving a star-shaped polygonal hole.</p><p><em>Step 5:</em> Connect the point $\mathbf{p}_i$ to all the vertices of this star-shaped polygon, and add the newly formed triangles to the open list and the triangle list stored with $\mathbf{p}_i$.</p><p><em>Step 6:</em> After all the points are inserted, merge the open list and the closed list into the Delaunay triangle list, obtain the Delaunay edge list from the Delaunay triangle list, and delete the edges from the Delaunay edge list that contains a vertex of the original “super” triangle.</p><p><strong><em>End</em></strong></p><h1 id="Time-Complexity-of-the-Algorithm"><a href="#Time-Complexity-of-the-Algorithm" class="headerlink" title="Time Complexity of the Algorithm"></a>Time Complexity of the Algorithm</h1><p>The optimized algorithm for Delaunay triangulation takes $O(n\log n)$ time. As Delaunay triangulation is a planar graph, the number of triangles incident on one point is constant, so the procedure of finding adjacent triangles takes constant time and the time of generating the Voronoi diagram is $O(n)$. Therefore, the total running time of this algorithm is $O(n\log n)$.</p><h1 id="Implementation-of-the-Algorithm"><a href="#Implementation-of-the-Algorithm" class="headerlink" title="Implementation of the Algorithm"></a>Implementation of the Algorithm</h1><p>This algorithm is implemented in C#. Figure 3 shows the result of the implementation of this algorithm, it is capable of handling massive input of 10000 points.</p><p><img src="/img/implementation_of_voronoi_diagram_and_delaunay_triangulation/implementation_of_voronoi_diagram_and_delaunay_triangulation_3.jpg" alt="implementation of voronoi diagram and delaunay triangulation figure 3"></p><center><b>Figure 3:</b> Voronoi diagram and Delaunay triangulation for 10000 points.</center><p>The GitHub repository of this implementation is <a href="https://github.com/IsaacGuan/Voronoi-Delaunay" target="_blank" rel="external">IsaacGuan/Voronoi-Delaunay</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Voronoi diagram&lt;/a&gt; and the &lt;a href=&quot;https://e
      
    
    </summary>
    
    
      <category term="computational-geometry" scheme="https://isaacguan.github.io/tags/computational-geometry/"/>
    
      <category term="voronoi-diagram" scheme="https://isaacguan.github.io/tags/voronoi-diagram/"/>
    
      <category term="delaunay-triangulation" scheme="https://isaacguan.github.io/tags/delaunay-triangulation/"/>
    
  </entry>
  
  <entry>
    <title>Width of a Set in the Plane</title>
    <link href="https://isaacguan.github.io/2017/11/19/Width-of-a-Set-in-the-Plane/"/>
    <id>https://isaacguan.github.io/2017/11/19/Width-of-a-Set-in-the-Plane/</id>
    <published>2017-11-19T15:19:12.000Z</published>
    <updated>2017-11-19T23:45:24.012Z</updated>
    
    <content type="html"><![CDATA[<p>The book <a href="http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/scan/CMU-CS-80-101.pdf" target="_blank" rel="external"><em>Geometric Transforms for Fast Geometric Algorithms</em></a> introduces an interesting algorithm from page 84 of computing the diameter of a set of points in two dimensions. Inspired by this, I come up with a solution to computing the width of a set using geometric transformation.</p><h1 id="Definition-of-the-Width-of-a-Set"><a href="#Definition-of-the-Width-of-a-Set" class="headerlink" title="Definition of the Width of a Set"></a>Definition of the Width of a Set</h1><p>Let $S$ be a set of $n$ points in $\mathbb{R}^2$. If $l$ and $l^\prime$ are two parallel lines, then the region between them is called a slab. The width of $S$ is defined to be the minimum distance between the bounding lines of any slab that contains all points of $S$.</p><h1 id="Characterization-of-the-Width"><a href="#Characterization-of-the-Width" class="headerlink" title="Characterization of the Width"></a>Characterization of the Width</h1><p>Before presenting the width-finding algorithm, I’d like first to characterize the width of a set of points, introduce some terminology, and prove several theorems.</p><p>When we say that $A$ is contained in $B$, it means that every point of $A$ also lies in $B$. The bounding lines of a slab could contain a vertex or an edge of the convex hull $H(S)$ of the set of points $S$.</p><p><strong>Theorem 1:</strong> Let $l$ and $l^\prime$ be two parallel lines that define the width of $S$, both $l$ and $l^\prime$ contain a vertex of the convex hull $H(S)$ of $S$.</p><p><strong>Proof.</strong> Assume otherwise. The width is determined by parallel lines $l$ and $l^\prime$, and $l$ contains a vertex of $H(S)$, $l^\prime$ does not, as shown in Figure 1. Then there must exist a line $l^{\prime\prime}$ closer to the set of points $S$ and produces a smaller distance. Q.E.D.</p><p><img src="/img/width_of_a_set_in_the_plane/width_of_a_set_in_the_plane_1.jpg" alt="width of a set in the plane figure 1"></p><center><b>Figure 1:</b> The width of a set.</center><p><strong>Theorem 2:</strong> Let $l$ and $l^\prime$ be two parallel lines that define the width of $S$, at least one of $l$ and $l^\prime$ contain an edge of the convex hull $H(S)$ of $S$.</p><p><strong>Proof.</strong> Assume otherwise. The width is determined by parallel lines $l$ and $l^\prime$ which both pass a vertex but neither of them passes an edge. We can rotate the parallel lines in preferred direction of rotation which means we rotate $l$ and $l^\prime$ about the two vertices they pass to form new parallel lines $l_1$ and ${l_1}^\prime$, and the distance between $l_1$ and ${l_1}^\prime$ is smaller. This contradicts the assumption. Q.E.D.</p><h1 id="Computing-the-Width"><a href="#Computing-the-Width" class="headerlink" title="Computing the Width"></a>Computing the Width</h1><p>We can divide the convex hull into 2 parts: the upper hull and the lower hull, so that when one of the parallel lines of support of $S$ meets the convex hull on the upper hull, the other is on the lower hull.</p><p>According to the <a href="https://en.wikipedia.org/wiki/Linear_programming#Duality" target="_blank" rel="external">duality transformation</a>, a non-vertical line $y=kx+b$ can be transformed into the point $(k,b)$ which is formed by the slope and intercept of the line. And a point $(a,b)$ can be transformed into the line $y=k(x-a)+b$ which means the set of lines that pass through it.</p><p>Hence, we define the transform of an edge of the convex hull $H(S)$ of $S$ as the slope of the line containing that edge and the transform of a vertex of $H(S)$ as the set of slopes of lines that pass through it. I.e., the line containing an edge $y=kx+b$ can be mapped to the one dimensional point $(k)$ and the vertex that lies between two edges $y=k_1x+b_1$ and $y=k_2x+b_2$ can be mapped to the closed interval $[k_1,k_2]$. As shown in Figure 2, the convex hull is transformed into a line that consists of an upper part and a lower part.</p><p><img src="/img/width_of_a_set_in_the_plane/width_of_a_set_in_the_plane_2.jpg" alt="width of a set in the plane figure 2"></p><center><b>Figure 2:</b> The transform.</center><p>As the slopes of the edges of the convex hull are already sorted, when we transform the convex hull to a line, the upper and lower sets of points and intervals on the line are in sorted order.</p><p>As known from Theorem 1, the parallel lines of support pass through an edge and a vertex of the convex hull $H(S)$, which means the corresponding point and interval pair contained in the parallel lines must intersect on the upper and lower lines, e.g., $\mathbf{p}_5$ and $l_8$, $\mathbf{p}_6$ and $l_2$ in Figure 2.</p><p>Thus, finding the width of a set of points $S$ can be reduced to scanning the intersections of point and interval pairs on the upper and lower hulls of its convex hull $H(S)$.</p><p>To summarize, the width of a set of points $S$ can be computed in the following manner.</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Construct the convex hull $H(S)$ of $S$.</p><p><em>Step 2:</em> Apply the transform to obtain two ordered sets of points and intervals.</p><p><em>Step 3:</em> Scan the sets for intersections between the intervals of one set and the points of the other, generating the corresponding vertex and edge pair. For each such pair, compute the distance between the vertex and the extended edge, and note the smallest such distance. When the scan is complete, that distance is the width.</p><p><strong><em>End</em></strong></p><h1 id="Time-Complexity"><a href="#Time-Complexity" class="headerlink" title="Time Complexity"></a>Time Complexity</h1><p>According to the <a href="https://en.wikipedia.org/wiki/Gift_wrapping_algorithm" target="_blank" rel="external">gift wrapping algorithm</a> and the <a href="https://en.wikipedia.org/wiki/Graham_scan" target="_blank" rel="external">Graham scan algorithm</a>, the running time of finding the convex hull of a set of points can be minimized to $O(n\log h)$, where $h$ is the number of points on the convex hull. And obtaining the two ordered sets and scanning the sets both takes $O(h)$ time. Therefore, finding the width of a set of points $S$ in $\mathbb{R}^2$ takes $O(n\log h)$ time.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The book &lt;a href=&quot;http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/scan/CMU-CS-80-101.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;em&gt;Geom
      
    
    </summary>
    
    
      <category term="computational-geometry" scheme="https://isaacguan.github.io/tags/computational-geometry/"/>
    
      <category term="convex-hull" scheme="https://isaacguan.github.io/tags/convex-hull/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://isaacguan.github.io/2017/11/01/Hello-World/"/>
    <id>https://isaacguan.github.io/2017/11/01/Hello-World/</id>
    <published>2017-11-01T20:59:25.000Z</published>
    <updated>2017-11-02T03:28:09.150Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Existence precedes essence.</p></blockquote><p>During the high school years, I once kept a diary, sort of artsy lifestyle possessed by the petty bourgeoisie. And this experience told me the truth that writing is really time-consuming, especially when you are making “sentimental twaddle” on trivial things in daily life.</p><p>Time flies. I haven’t been keeping that habit for a long time and I will be 23 this month. Like every young man in his 20s, I feel at a loss from time to time. Looking back on the past few years, seemingly I have gone through a lot of things and met up with a lot of people, but the future still remains uncertain.</p><p>Some people of this era are distressed and tend to explore the meaning of their existence. That is why I am quoting the words of <a href="https://en.wikipedia.org/wiki/Jean-Paul_Sartre" target="_blank" rel="external">Jean-Paul Sartre</a> at the beginning of this post. It tells that life could be meaningless, according to my understanding, unless you create yourself a meaning.</p><p>But I cannot say that I am an existentialist. An existentialist should be fearless, like <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche" target="_blank" rel="external">Friedrich Nietzsche</a>, who asserted that <em>God is dead</em>. I am just following the guidance of these great philosophers to add something that I believe is meaningful to my life.</p><p>Thus, I decided to build this place and record something of myself, which generally includes:</p><ul><li>Something about my study (algorithms, programming, projects I am doing, etc.).</li><li>Something about my hobby (traveling, cycling, reading, photographing, etc.).</li><li>And more possibilities…</li></ul><p>It is lucky for me to live in this digital age so that I can easily build up this small website on my own, using <a href="https://pages.github.com/" target="_blank" rel="external">Github Pages</a> as the blogging platform and <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a> as the framework. I also would like to express my thanks to <a href="https://github.com/fi3ework" target="_blank" rel="external">fi3ework</a> who designed this beautiful theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank" rel="external">archer</a> that I am using.</p><p>Anyhow, this is the very first post of my blog. Wish me happy blogging :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Existence precedes essence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;During the high school years, I once kept a diary, sort of artsy lifestyle 
      
    
    </summary>
    
    
      <category term="hello-world" scheme="https://isaacguan.github.io/tags/hello-world/"/>
    
  </entry>
  
</feed>
