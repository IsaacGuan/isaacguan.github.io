<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Isaac&#39;s Blog</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://isaacguan.github.io/"/>
  <updated>2018-06-26T22:16:16.650Z</updated>
  <id>https://isaacguan.github.io/</id>
  
  <author>
    <name>Yanran Guan</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Plane Detection Using Machine Learning Approach</title>
    <link href="https://isaacguan.github.io/2018/06/01/Plane-Detection-Using-Machine-Learning-Approach/"/>
    <id>https://isaacguan.github.io/2018/06/01/Plane-Detection-Using-Machine-Learning-Approach/</id>
    <published>2018-06-01T15:09:59.000Z</published>
    <updated>2018-06-26T22:16:16.650Z</updated>
    
    <content type="html"><![CDATA[<p>Plane detection is a widely used technique that can be applied in many applications, e.g., augmented reality, where we have to detect a plane to generate AR models, and 3D scene reconstruction, especially for man-made scenes, which consist of many planar objects. Nowadays, with proliferation of acquisitive devices, deriving a massive point cloud is not a difficult task, which shows promise in doing plane detection in 3D point clouds.</p><h1 id="Related-Work"><a href="#Related-Work" class="headerlink" title="Related Work"></a>Related Work</h1><p>There are some existing plane detection and point cloud machine learning approaches proposed by recent researches.</p><h2 id="Plane-Detection-in-3D-Data"><a href="#Plane-Detection-in-3D-Data" class="headerlink" title="Plane Detection in 3D Data"></a>Plane Detection in 3D Data</h2><p>The <a href="https://robotik.informatik.uni-wuerzburg.de/telematics/download/3dresearch2011.pdf" target="_blank" rel="external">3D Hough transform</a> is one possible approach for doing plane detection. As well as line detection in 2D space, planes can be parameterized into a 3D Hough space. The <a href="https://github.com/amonszpart/RAPter" target="_blank" rel="external">RAPter</a> is another method that can be used for plane detection. It finds out the planes in a scene according to the predefined inter-plane relations, so RAPter is efficient for man-made scenes with significant inter-plane relations, but not adequate for some more general cases.</p><h2 id="Deep-Learning-for-3D-Data-and-Point-Cloud"><a href="#Deep-Learning-for-3D-Data-and-Point-Cloud" class="headerlink" title="Deep Learning for 3D Data and Point Cloud"></a>Deep Learning for 3D Data and Point Cloud</h2><p>People have designed many machine learning approaches on different representations of 3D data. For instance, the <a href="https://github.com/dimatura/voxnet" target="_blank" rel="external">volumetric CNN</a> consumes volumetric data as input, and apply 3D convolutional neural networks on voxelized shapes. The <a href="https://github.com/suhangpro/mvcnn" target="_blank" rel="external">multiview CNN</a> projects the 3D shape into 2D images, and then apply 2D convolutional neural networks to classify them. The <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/app/2A_011.pdf" target="_blank" rel="external">feature-based DNN</a> focuses on generating a shape vector of the object according to its traditional shape features, and then use a fully connected net to classify the shape. The <a href="https://github.com/charlesq34/pointnet" target="_blank" rel="external">PointNet</a> proposed a new design of neural network based on symmetric functions that can take unordered input of point clouds.</p><h1 id="Design-of-Neural-Networks"><a href="#Design-of-Neural-Networks" class="headerlink" title="Design of Neural Networks"></a>Design of Neural Networks</h1><p>PointNet uses symmetric functions that can effectively capture the global features of a point cloud. Inspired by this, this experiment uses a symmetric network that concatenates global features and local features. Besides, for comparison, I also used a traditional network that simply generates a high dimensional local feature space by multilayer perceptions.</p><h2 id="Symmetric-Network"><a href="#Symmetric-Network" class="headerlink" title="Symmetric Network"></a>Symmetric Network</h2><p>According to the universal approximation of symmetric function proposed by PointNet, a symmetric function $f$ can be arbitrarily approximated by a composition of a set of single variable functions and a max pooling function, as described in Theorem 1.</p><p><strong>Theorem 1:</strong> Suppose $f\colon\chi\to\mathbb{R}$ is a continuous set function w.r.t. Hausdorff distance $d_H$. $\forall\epsilon&gt;0$, $\exists$ a continuous function $h$ and a symmetric function $g(\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n)=\gamma\circ\max$, such that for any $S\in\chi$,$$\begin{equation}|f(S)-\gamma(\max_{\mathbf{x}_i\in S}\{h(\mathbf{x}_i)\})|&lt;\epsilon,\tag{1}\label{eq:1}\end{equation}$$where $\mathbf{x}_1,\mathbf{x}_2,\ldots,\mathbf{x}_n$ is the full list of elements in $S$ ordered arbitrarily, $\gamma$ is a continuous function, and $\max$ is a vector max operator that takes $n$ vectors as input and returns a new vector of the element-wise maximum.</p><p>Thus, according to Theorem 1, the symmetric network can be designed as a multi-layer perceptron network connected with a max pooling function, which is shown in Figure 1.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_1.jpg" alt="plane detection using machine learning approach figure 1"></p><center><b>Figure 1:</b> Architecture of the symmetric network.</center><p>In order to achieve the invariance towards geometric transformation, an input alignment ($\mathbf{T}_1$ in Figure 1) and a feature alignment ($\mathbf{T}_2$ in Figure 1) are respectively applied on the input space and feature space. The points are first mapped to a 64 dimensional feature space point, and then mapped to a 1024 dimensional feature space. A max pooling function is applied on the 1024 dimensional feature space to generate a 1024 length global feature vector. The global vector is then concatenated to the 64 dimensional feature space which generates a 1088 dimensional space. Lastly, a 2 dimensional vector for each point, which represents the score for planar part and non-planar part, is updated from the 1088 dimensional space.</p><h2 id="Asymmetric-Network"><a href="#Asymmetric-Network" class="headerlink" title="Asymmetric Network"></a>Asymmetric Network</h2><p>For doing comparison, a traditional asymmetric network is also introduced in this experiment. It is basically modified from the symmetric network by detaching the max pooling function from the multi-layer perceptron network. The architecture of the asymmetric network is shown in Figure 2.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_2.jpg" alt="plane detection using machine learning approach figure 2"></p><center><b>Figure 2:</b> Architecture of the asymmetric network.</center><p>Instead of concatenating the global feature vector to the 64 dimensional feature space, the asymmetric network simply concatenates the 64 dimensional feature space and the 1024 dimensional feature space, and generates the 2 dimensional scores from that.</p><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><p>The experiment is conducted in the following pattern: first, prepare the data for training and testing; then feed the training data respectively to the symmetric network and asymmetric network, and find the best trained model according to the minimum total loss; lastly, compare the plane detection results of symmetric network and asymmetric network.</p><h2 id="Experimental-Data-and-Data-Preprocessing"><a href="#Experimental-Data-and-Data-Preprocessing" class="headerlink" title="Experimental Data and Data Preprocessing"></a>Experimental Data and Data Preprocessing</h2><p>This experiment uses data from the <a href="http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html" target="_blank" rel="external">ShapeNetPart</a> dataset. I chose 64 tables, which have a significant planar surface, from the table repository for training, and 8 for testing and evaluation.</p><p>Each the point cloud was previously undersampled to a size of 2048 points, using a random sampler. The point clouds for training were written into an HDF5 file, which contains 2 views, <code>points</code> and <code>pid</code>, respectively recording the point coordinate and the planar information associated with each point.</p><p>The planar part were marked out manually on the original data. Figure 3 shows a few examples of table objects for training.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_3.jpg" alt="plane detection using machine learning approach figure 3"></p><center><b>Figure 3:</b> A part of the training data.</center><h2 id="Best-Trained-Models"><a href="#Best-Trained-Models" class="headerlink" title="Best Trained Models"></a>Best Trained Models</h2><p>Basically, a point cloud contains more points in non-planar part than points in planar part. In order to handle this unbalanced data, I used a weighted cross entropy function to calculate the mean loss for each epoch. The planar part is assigned with a weight of 0.7 and the non-planar part is assigned with a weight of 0.3.</p><p>Both networks are trained for 150 epochs. The plot of mean loss for training the symmetric network is shown in Figure 4.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_4.jpg" alt="plane detection using machine learning approach figure 4"></p><center><b>Figure 4:</b> Total mean loss per epoch for the training of symmetric network.</center><p>According to Figure 4, total mean loss is stuck at a very low value after the 100th epoch, so I chose trained model from the 130th epoch for testing.</p><p>Besides, Figure 5 is the plot of mean loss for training the asymmetric network.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_5.jpg" alt="plane detection using machine learning approach figure 5"></p><center><b>Figure 5:</b> Total mean loss per epoch for the training of asymmetric network.</center><p>The loss value stays low after the 100th epoch, and there is a fluctuation around the 130th epoch. Such fluctuation may be caused by overfitting, so I chose the trained model from 110th epoch for testing.</p><h2 id="Experiment-Results"><a href="#Experiment-Results" class="headerlink" title="Experiment Results"></a>Experiment Results</h2><p>The testing set has a size of 8 objects. The testing result on the model of symmetric network shows an accuracy of 83.4534% and an <a href="https://en.wikipedia.org/wiki/Jaccard_index" target="_blank" rel="external">Intersection over Union (IoU)</a> of 71.1421%. Figure 6 illustrates the plane detection result on the testing set using the model generated by symmetric network.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_6.jpg" alt="plane detection using machine learning approach figure 6"></p><center><b>Figure 6:</b> Testing results of the symmetric network.</center><p>The result shows a fairly good performance of neural networks doing plane detection for objects from single category. The accuracy could even possibly rise if larger training set is prepared. The model seems to favor a table object with a more normal shape, i.e., a table with a square tabletop and four straight legs. For tables without a regular shape, the classification accuracy is relatively lower, and the model tends to misclassify the points in the middle of the table top.</p><p>The asymmetric network shows a similar testing result, which comes out with an accuracy of 85.7117% and an IoU of 75.0279%. The plane detection results of the asymmetric network is shown in the Figure 7 below.</p><p><img src="/img/plane_detection_using_machine_learning_approach/plane_detection_using_machine_learning_approach_7.jpg" alt="plane detection using machine learning approach figure 7"></p><center><b>Figure 7:</b> Testing results of the symmetric network.</center><p>Similar to the results of the symmetric network, the model based on the asymmetric network shows a good performance on objects with a more regular shape. It may also fail to classify the points in the middle of the tabletop. Furthermore, the asymmetric network could also misclassify a few points on the legs of a table, which is shown in the 1st, 2nd, and 8th objects, and such pattern is not observed in the results of symmetric network.</p><h1 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h1><p>In this experiment, although the asymmetric network shows a slightly higher classification accuracy, we cannot conclude that asymmetric network has a better performance for doing the plane detection work. Since only one category of object is experimented, the global vector generated in the symmetric function actually does not make an effort. In the further experiments, we can introduce more categories of objects and see how the networks will work on more complicated shapes.</p><p>This experiment shows the potential of neural networks for doing plane detection. According to the experiment results, misclassification is resulting in holes on the detected planes. This is a not a very severe problem for plane detection tasks, as we can apply a 3D Hough transform afterwards, which is robust to missing and contaminated data, on the points of detected planar part to generate the plane information.</p><p>All the code and data of this experiment can be found over my GitHub repository: <a href="https://github.com/IsaacGuan/PointNet-Plane-Detection" target="_blank" rel="external">IsaacGuan/PointNet-Plane-Detection</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Plane detection is a widely used technique that can be applied in many applications, e.g., augmented reality, where we have to detect a p
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="plane-detection" scheme="https://isaacguan.github.io/tags/plane-detection/"/>
    
      <category term="machine-learning" scheme="https://isaacguan.github.io/tags/machine-learning/"/>
    
      <category term="pointnet" scheme="https://isaacguan.github.io/tags/pointnet/"/>
    
  </entry>
  
  <entry>
    <title>Prepare Your Own Data for PointNet</title>
    <link href="https://isaacguan.github.io/2018/05/04/Prepare-Your-Own-Data-for-PointNet/"/>
    <id>https://isaacguan.github.io/2018/05/04/Prepare-Your-Own-Data-for-PointNet/</id>
    <published>2018-05-05T00:15:58.000Z</published>
    <updated>2018-05-15T02:02:56.270Z</updated>
    
    <content type="html"><![CDATA[<p>Being a novel deep net architecture invariant towards input order, <a href="https://github.com/charlesq34/pointnet" target="_blank" rel="external">PointNet</a> is able to consume unordered point clouds directly and thus has a promising prospect in the field of geometry processing. At present, the most popular implementation of PointNet is based on <a href="https://www.tensorflow.org/" target="_blank" rel="external">TensorFlow</a> and it takes <a href="https://support.hdfgroup.org/HDF5/" target="_blank" rel="external">HDF5</a> as standard input format. It could be a bit confusing for people converting point clouds to HDF5 files and this article is about to tell you how to collect HDF5 datasets for PointNet learning.</p><h1 id="PTS-Data"><a href="#PTS-Data" class="headerlink" title="PTS Data"></a>PTS Data</h1><p>We can download raw data from a certain 3D data repositories, for instance, the <a href="http://web.stanford.edu/~ericyi/project_page/part_annotation/index.html" target="_blank" rel="external">ShapeNetPart</a> dataset. The data directly derived from those repositories is basically in the <a href="http://filext.com/file-extension/PTS" target="_blank" rel="external">PTS</a> file format, which is a set of unordered point coordinates with no headers or trailers. This actually makes things easier, as we can directly read the PTS file line by line and store the point cloud into an array <code>lines</code>. For example, before generating HDF5 datasets, we want that each point cloud has the same length. Thus, a simple subsampler can be applied on the PTS files. The following code snippet shows a random sampler subsampling the point cloud to 2048 points.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">f = open(file, <span class="string">'r+'</span>)</div><div class="line">lines = [line.rstrip() <span class="keyword">for</span> line <span class="keyword">in</span> f]</div><div class="line">slice = random.sample(lines, <span class="number">2048</span>)</div></pre></td></tr></table></figure><h1 id="PLY-Data"><a href="#PLY-Data" class="headerlink" title="PLY Data"></a>PLY Data</h1><p><a href="http://paulbourke.net/dataformats/ply/" target="_blank" rel="external">PLY</a> is a very famous file format that stores 3D data. It has headers to specify the variation and elements of the PLY file. Thus it could be a bit more complicated to deal with such data than PTS data. Luckily, we can find some ready made tools to read PLY files, e.g., the <a href="https://github.com/dranjan/python-plyfile" target="_blank" rel="external">plyfile</a>, which is able to read the numerical data from the PLY file as a <a href="http://www.numpy.org/" target="_blank" rel="external">NumPy</a> structured array. The installation of this tool is pretty easy, we can get it directly via <a href="https://pypi.org/project/pip/" target="_blank" rel="external">pip</a>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install plyfile</div></pre></td></tr></table></figure><p>For sure, prior to this, we should also have the NumPy installed.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install numpy</div></pre></td></tr></table></figure><p>The deserialization and serialization of PLY file data is done through <code>PlyData</code> and <code>PlyElement</code> instances, so we have to first import them. Besides, the NumPy module is also need to be loaded.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> plyfile <span class="keyword">import</span> PlyData, PlyElement</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div></pre></td></tr></table></figure><p>Then we can start to read a PLY file. Concretely, it looks like this.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">plydata = PlyData.read(filename + <span class="string">'.ply'</span>)</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">0</span>, plydata.elements[<span class="number">0</span>].count):</div><div class="line">    data[i] = [plydata[<span class="string">'vertex'</span>][<span class="string">'x'</span>][i], plydata[<span class="string">'vertex'</span>][<span class="string">'y'</span>][i], plydata[<span class="string">'vertex'</span>][<span class="string">'z'</span>][i]]</div></pre></td></tr></table></figure><h1 id="Write-HDF5-File"><a href="#Write-HDF5-File" class="headerlink" title="Write HDF5 File"></a>Write HDF5 File</h1><p>We use the <a href="https://github.com/h5py/h5py" target="_blank" rel="external">h5py</a> package as the interface to the HDF5 data format.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install libhdf5-dev</div><div class="line">pip install h5py</div></pre></td></tr></table></figure><p>We first import this package.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> h5py</div></pre></td></tr></table></figure><p>For creating a HDF5 file, we use the <code>h5py.File</code> function to initialize it, which takes two arguments. The first argument provides the filename and location, the second the mode. We’re writing the file, so we provide a <code>w</code> for write access.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">f = h5py.File(<span class="string">'data.h5'</span>, <span class="string">'w'</span>)</div></pre></td></tr></table></figure><p>Then we need to define the shape and type of the data to write to the HDF5 file.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">data = np.zeros((<span class="number">128</span>, <span class="number">2048</span>, <span class="number">3</span>), dtype = np.uint8)</div></pre></td></tr></table></figure><p>After filling <code>data</code> with the point clouds information read from the PTS or PLY files, we can write it to the HDF5 file <code>f</code>, using the <code>create_dataset</code> function associated to it, where we provide a name for the dataset, and the NumPy array.</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">f.create_dataset(<span class="string">'data'</span>, data = data)</div></pre></td></tr></table></figure><p>I have a very concrete example of providing data for PointNet in my GitHub repository <a href="https://github.com/IsaacGuan/PointNet-Plane-Detection" target="_blank" rel="external">IsaacGuan/PointNet-Plane-Detection</a>. We can take the script <a href="https://github.com/IsaacGuan/PointNet-Plane-Detection/blob/master/data/write_hdf5.py" target="_blank" rel="external">write_hdf5.py</a> as a reference.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Being a novel deep net architecture invariant towards input order, &lt;a href=&quot;https://github.com/charlesq34/pointnet&quot; target=&quot;_blank&quot; rel=&quot;
      
    
    </summary>
    
    
      <category term="pointnet" scheme="https://isaacguan.github.io/tags/pointnet/"/>
    
      <category term="data-preprocessing" scheme="https://isaacguan.github.io/tags/data-preprocessing/"/>
    
  </entry>
  
  <entry>
    <title>RAPter on Ubuntu</title>
    <link href="https://isaacguan.github.io/2018/04/29/RAPter-on-Ubuntu/"/>
    <id>https://isaacguan.github.io/2018/04/29/RAPter-on-Ubuntu/</id>
    <published>2018-04-29T12:12:16.000Z</published>
    <updated>2018-04-29T15:51:17.745Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://github.com/amonszpart/RAPter" target="_blank" rel="external">RAPter</a> gives us a very nice design and implementation for doing man-made scene reconstruction. According to the nature of its algorithm, it also shows a potential use in terms of 3D plane detection. However, the documentation related to it is pretty poor, so I decide to write here a summary of how to install it on <a href="http://releases.ubuntu.com/16.04/" target="_blank" rel="external">Ubuntu 16.04 LTS</a>.</p><h1 id="Workspace"><a href="#Workspace" class="headerlink" title="Workspace"></a>Workspace</h1><p>The first thing is to create a <code>workspace</code> directory in user’s home directory and a <code>3rdparty</code> subdirectory under <code>workspace</code>. Thus we can define the workspace path <code>WORKSPACE_DIR</code> as <code>$ENV{HOME}/workspace</code> and the third party dependencies path <code>THIRD_PARTY_DIR</code> as <code>${WORKSPACE_DIR}/3rdparty</code>. In the following steps, we will store all the implementations of RAPter tools and experimental data in <code>workspace</code> and download all the dependencies in <code>3rdparty</code>.</p><h1 id="Dependencies"><a href="#Dependencies" class="headerlink" title="Dependencies"></a>Dependencies</h1><p>RAPter requires several dependencies, including <a href="https://opencv.org/" target="_blank" rel="external">OpenCV</a>, <a href="http://pointclouds.org/" target="_blank" rel="external">PointCloudLibrary</a>, <a href="https://projects.coin-or.org/svn/Bonmin/" target="_blank" rel="external">CoinBonmin</a>, and <a href="https://github.com/mkirchner/libfbi" target="_blank" rel="external">libfbi</a>. Prior to downloading and installing all these dependences, we need to first get prepared with a few tools, including <a href="https://git-scm.com/" target="_blank" rel="external">Git</a>, <a href="https://subversion.apache.org/" target="_blank" rel="external">SVN</a>, and <a href="https://subversion.apache.org/" target="_blank" rel="external">CMake</a> via the following commands.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">sudo apt-get update</div><div class="line">sudo apt-get install git</div><div class="line">sudo apt-get install subversion</div><div class="line">sudo apt-get install cmake</div></pre></td></tr></table></figure><h2 id="OpenCV"><a href="#OpenCV" class="headerlink" title="OpenCV"></a>OpenCV</h2><p>To install the latest version of OpenCV, we can use the installation script <a href="https://github.com/milq/milq/blob/master/scripts/bash/install-opencv.sh" target="_blank" rel="external">install-opencv.sh</a>. We first download this script in the <code>3rdparty</code> directory, and then execute:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">bash install-opencv.sh</div></pre></td></tr></table></figure><p>Type the <code>sudo</code> password and OpenCV will be installed.</p><h2 id="PointCloudLibrary"><a href="#PointCloudLibrary" class="headerlink" title="PointCloudLibrary"></a>PointCloudLibrary</h2><p>There is no ready-made script for PointCloudLibrary, so the installation of it could be a bit tedious.</p><h3 id="Setup-Prerequisites"><a href="#Setup-Prerequisites" class="headerlink" title="Setup Prerequisites"></a>Setup Prerequisites</h3><p>The PointCloudLibrary also needs a bunch of prerequisite tools. We use the commands below to have them setup.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install git build-essential linux-libc-dev</div><div class="line">sudo apt-get install cmake cmake-gui </div><div class="line">sudo apt-get install libusb-1.0-0-dev libusb-dev libudev-dev</div><div class="line">sudo apt-get install mpi-default-dev openmpi-bin openmpi-common  </div><div class="line">sudo apt-get install libflann1.8 libflann-dev</div><div class="line">sudo apt-get install libeigen3-dev</div><div class="line">sudo apt-get install libboost-all-dev</div><div class="line">sudo apt-get install libvtk5.10-qt4 libvtk5.10 libvtk5-dev</div><div class="line">sudo apt-get install libqhull* libgtest-dev</div><div class="line">sudo apt-get install freeglut3-dev pkg-config</div><div class="line">sudo apt-get install libxmu-dev libxi-dev </div><div class="line">sudo apt-get install mono-complete</div><div class="line">sudo apt-get install qt-sdk openjdk-8-jdk openjdk-8-jre</div></pre></td></tr></table></figure><h3 id="Build-PointCloudLibrary"><a href="#Build-PointCloudLibrary" class="headerlink" title="Build PointCloudLibrary"></a>Build PointCloudLibrary</h3><p>In the <code>3rdparty</code> directory, PointCloudLibrary is obtained by:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/PointCloudLibrary/pcl.git</div></pre></td></tr></table></figure><p>Now we get into <code>pcl</code>, create a <code>release</code> directory in it, and then follow the <code>cmake</code> build process.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> pcl</div><div class="line">mkdir release</div><div class="line"><span class="built_in">cd</span> release</div><div class="line">cmake -DCMAKE_BUILD_TYPE=None -DCMAKE_INSTALL_PREFIX=/usr \</div><div class="line">      -DBUILD_GPU=ON -DBUILD_apps=ON -DBUILD_examples=ON \</div><div class="line">      -DCMAKE_INSTALL_PREFIX=/usr ..</div><div class="line">make</div></pre></td></tr></table></figure><p>Once the build is finished, we install it by:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">make install</div></pre></td></tr></table></figure><h2 id="CoinBonmin"><a href="#CoinBonmin" class="headerlink" title="CoinBonmin"></a>CoinBonmin</h2><p>As usual, we need to intall the prerequisites:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">sudo apt-get install liblapack-dev</div><div class="line">sudo apt-get install libblas-dev</div><div class="line">sudo apt-get install fortran-compiler</div></pre></td></tr></table></figure><p>Then we download CoinBonmin in <code>3rdparty</code>:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">svn co https://projects.coin-or.org/svn/Bonmin/stable/1.5 CoinBonmin-stable</div></pre></td></tr></table></figure><p>CoinBonmin also requires a third party solver. To get it, we have to first go to the directory <code>CoinBonmin-stable/ThirdParty/Mumps</code> and run:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./get.Mumps</div></pre></td></tr></table></figure><p>To compile and install CoinBonmin, we have to go back to the root directory of it, which is <code>CoinBonmin-stable</code>, and create a <code>build</code> directory in it, then run the <code>cmake</code> process.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">mkdir build</div><div class="line"><span class="built_in">cd</span> build</div><div class="line">../configure -C</div><div class="line">make</div><div class="line">make install</div></pre></td></tr></table></figure><h2 id="libfbi"><a href="#libfbi" class="headerlink" title="libfbi"></a>libfbi</h2><p>We only have to download libfbi in <code>3rdparty</code>, no prerequisites or installation is required.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/mkirchner/libfbi.git</div></pre></td></tr></table></figure><h1 id="Compilation"><a href="#Compilation" class="headerlink" title="Compilation"></a>Compilation</h1><p>Finally, all the dependencies are satisfied. We first get the RAPter in the <code>workspace</code> directory:</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/amonszpart/RAPter.git</div></pre></td></tr></table></figure><p>Then get into the root of Rapter, which is <code>RAPter/RAPter</code>, and build it.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">mkdir build</div><div class="line"><span class="built_in">cd</span> build</div><div class="line">cmake -DCMAKE_BUILD_TYPE=Release ..</div><div class="line">make</div></pre></td></tr></table></figure><p>Besides, we may also need to change the default directories of dependencies in CMakelist.txt. For example, following this article, the directory of PointCloudLibrary <code>PCL_DIR</code> should be <code>${THIRD_PARTY_DIR}/pcl/</code> instead of <code>${THIRD_PARTY_DIR}/pcl-trunk2/install/share/pcl-1.8/</code>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The &lt;a href=&quot;https://github.com/amonszpart/RAPter&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;RAPter&lt;/a&gt; gives us a very nice design and implementati
      
    
    </summary>
    
    
      <category term="ubuntu" scheme="https://isaacguan.github.io/tags/ubuntu/"/>
    
      <category term="rapter" scheme="https://isaacguan.github.io/tags/rapter/"/>
    
  </entry>
  
  <entry>
    <title>Tutte Embedding for Parameterization</title>
    <link href="https://isaacguan.github.io/2018/03/19/Tutte-Embedding-for-Parameterization/"/>
    <id>https://isaacguan.github.io/2018/03/19/Tutte-Embedding-for-Parameterization/</id>
    <published>2018-03-19T21:36:33.000Z</published>
    <updated>2018-03-27T01:28:50.803Z</updated>
    
    <content type="html"><![CDATA[<p>In terms of geometry processing, <a href="https://en.wikipedia.org/wiki/Tutte_embedding" target="_blank" rel="external">Tutte embedding</a>, also known as barycentric embedding, can be used for <a href="https://en.wikipedia.org/wiki/Mesh_parameterization" target="_blank" rel="external">mesh parameterization</a> by fixing the boundary vertices of the mesh on a certain convex polygon, and building a <a href="https://en.wikipedia.org/wiki/Tutte_embedding" target="_blank" rel="external">crossing-free straight-line embedding</a> with the interior vertices inside the convex boundary.</p><h1 id="Implementation-of-Tutte-Embedding"><a href="#Implementation-of-Tutte-Embedding" class="headerlink" title="Implementation of Tutte Embedding"></a>Implementation of Tutte Embedding</h1><p>In order to implement a Tutte embedding, we have to use two vectors $\mathbf{u}$ and $\mathbf{v}$ to store the parameterized coordinates, and another two vectors $\bar{\mathbf{u}}$ and $\bar{\mathbf{v}}$ to store the boundary vertices. For the interior vertex $i$, we can directly applying the following equation on it to build the barycentric mapping:$$\begin{equation}\sum_{}a_{i,j}\begin{pmatrix}u_j\\v_j\end{pmatrix}-a_{i,i}\begin{pmatrix}u_i\\v_i\end{pmatrix}=0,\tag{1}\label{eq:1}\end{equation}$$where $j$ denotes the adjacent vertices of $i$.</p><p>For those vertices on the boundary, we use the equation below and set $a_{i,i}=1$, to have them fixed on a convex shape:$$\begin{equation}\sum_{}a_{i,i}\begin{pmatrix}u_i\\v_i\end{pmatrix}=\begin{pmatrix}\bar{u}_i\\\bar{v}_i\end{pmatrix}.\tag{2}\label{eq:2}\end{equation}$$</p><p>Thus, we can build the two linear systems below:$$\begin{equation}\begin{cases}\mathbf{A}\mathbf{u}=\bar{\mathbf{u}},\\\mathbf{A}\mathbf{v}=\bar{\mathbf{v}},\end{cases}\tag{3}\label{eq:3}\end{equation}$$where $\mathbf{A}=(a_{i,j})$ is a sparse matrix storing all the weights $a_{i,j}$. And we can find the parameterized coordinates by solving for the two linear systems.</p><p>I chose a disk as the convex shape for fixing the boundary vertices. And I am still using <a href="https://github.com/GeometryCollective/geometry-processing-js" target="_blank" rel="external">geometry-processing-js</a> as mesh library, which is the same library I used in <a href="https://isaacguan.github.io/2018/01/26/Mesh-Smoothing/">Mesh Smoothing</a>.</p><h1 id="Weighting-Schemes"><a href="#Weighting-Schemes" class="headerlink" title="Weighting Schemes"></a>Weighting Schemes</h1><p>I tried three different weight sets, namely the uniform Laplacian weights, the Laplace-Beltrami weights, and the mean value weights.</p><h2 id="Uniform-Laplacian-Weights"><a href="#Uniform-Laplacian-Weights" class="headerlink" title="Uniform Laplacian Weights"></a>Uniform Laplacian Weights</h2><p>The uniform Laplacian weight set is easiest to build, but it does not take the geometric feature of the mesh into consideration. We can simply set the entries of the weight matrix in the following manner:$$\begin{equation}\begin{cases}a_{i,j}=1,\\a_{i,i}=-\sum_{}a_{i,j}=-D_i,\end{cases}\tag{4}\label{eq:4}\end{equation}$$where $D_i$ is the degree of vertex $i$, i.e., the number of edges connecting $i$.</p><h2 id="Laplace-Beltrami-Weights"><a href="#Laplace-Beltrami-Weights" class="headerlink" title="Laplace-Beltrami Weights"></a>Laplace-Beltrami Weights</h2><p>We can also derive weights from the <a href="https://en.wikipedia.org/wiki/Laplace%E2%80%93Beltrami_operator" target="_blank" rel="external">Laplace-Beltrami operator</a>:$$\begin{equation}\begin{cases}a_{i,j}=\frac{1}{2A_i}(\cot\alpha_{i,j}+\cot\beta_{i,j}),\\a_{i,i}=-\sum_{}a_{i,j},\end{cases}\tag{5}\label{eq:5}\end{equation}$$where $A_i$ denotes the area of the <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" target="_blank" rel="external">Voronoi region</a> of the vertex $i$, $\alpha_{i,j}$ and $\beta_{i,j}$ denote respectively the two corners opposite to the edge $(i,j)$, as shown in Figure 1.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_1.jpg" alt="tutte embedding for parameterization figure 1"></p><center><b>Figure 1:</b> Voronoi region of one vertex.</center><p>For calculating the value of $A_i$, we can use:$$\begin{equation}A_i=\frac{1}{8}\sum_{}(\cot\alpha_{i,j}+\cot\beta_{i,j})\|\mathbf{x}_i-\mathbf{x}_j\|^2,\tag{6}\label{eq:6}\end{equation}$$where $\|\mathbf{x}_i-\mathbf{x}_j\|$ is the length of edge $(i,j)$.</p><p>As far as one triangle from the one-ring neighborhood of a vertex is concerned, which is shown in Figure 2, the Voronoi region inside that triangle can be calculated as:$$\begin{equation}W=\frac{1}{8}\sum_{}(\|\mathbf{x}_i-\mathbf{x}_j\|^2\cot\alpha+\|\mathbf{x}_i-\mathbf{x}_j\|^2\cot\beta).\tag{7}\label{eq:7}\end{equation}$$</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_2.jpg" alt="tutte embedding for parameterization figure 2"></p><center><b>Figure 2:</b> Voronoi region of one triangle of one-ring neighborhood.</center><p>As the data structure of mesh is based on <a href="https://en.wikipedia.org/wiki/Doubly_connected_edge_list" target="_blank" rel="external">half-edge</a>, we can simply find the neighbor of a certain half-edge by retrieving its previous half-edge, e.g., in Figure 2, half-edge $\langle j_2,i\rangle$ is previous to half-edge $\langle i,j_1\rangle$. Thus, by traversing outgoing half-edges associated on a vertex, we can easily calculate the Voronoi region of it using the following code snippet:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> area = <span class="number">0.0</span>;</div><div class="line"><span class="keyword">for</span> (<span class="keyword">let</span> h <span class="keyword">of</span> v.adjacentHalfedges()) &#123;</div><div class="line">    <span class="keyword">let</span> u2 = <span class="keyword">this</span>.vector(h.prev).norm2();</div><div class="line">    <span class="keyword">let</span> v2 = <span class="keyword">this</span>.vector(h).norm2();</div><div class="line">    <span class="keyword">let</span> cotAlpha = <span class="keyword">this</span>.cotan(h.prev);</div><div class="line">    <span class="keyword">let</span> cotBeta = <span class="keyword">this</span>.cotan(h);</div><div class="line">    area += (u2 * cotAlpha + v2 * cotBeta) / <span class="number">8</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><h2 id="Mean-Value-Weights"><a href="#Mean-Value-Weights" class="headerlink" title="Mean Value Weights"></a>Mean Value Weights</h2><p>The Laplace-Beltrami weight takes the geometry of a mesh into account, but it could be negative with obtuse triangles. Based on the mean value theorem, we can derive the mean value weights, which are always positive and generate one-to-one mappings:$$\begin{equation}\begin{cases}a_{i,j}=\frac{1}{\|\mathbf{x}_i-\mathbf{x}_j\|}(\tan\frac{\delta_{i,j}}{2}+\tan\frac{\gamma_{i,j}}{2}),\\a_{i,i}=-\sum_{}a_{i,j},\end{cases}\tag{8}\label{eq:8}\end{equation}$$where $\|\mathbf{x}_i-\mathbf{x}_j\|$ is the length of edge $(i,j)$, $\delta_{i,j}$ and $\gamma_{i,j}$ are two neighboring corners on both sides of edge $(i,j)$, as shown in Figure 3.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_3.jpg" alt="tutte embedding for parameterization figure 3"></p><center><b>Figure 3</b></center><p>In the real implementation, like what we did in calculating the Voronoi region, we can still take advantage of the sequence of half-edges to find the corresponding corners:</p><figure class="highlight javascript"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">let</span> delta = h.next.corner;</div><div class="line"><span class="keyword">let</span> gamma = h.twin.prev.corner;</div></pre></td></tr></table></figure><h1 id="Results-and-Conclusion"><a href="#Results-and-Conclusion" class="headerlink" title="Results and Conclusion"></a>Results and Conclusion</h1><p>The results of Tutte embedding is shown in Figure 4, applied on different objects with different weight sets.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_4.jpg" alt="tutte embedding for parameterization figure 4"></p><center><b>Figure 4:</b> Results of Tutte embedding.</center><p>We can conclude from the results that the Laplace-Beltrami weight set and mean value weight set can better preserve the shape of triangles in the original mesh. This leads to the interior holes of a mesh to enlarge, which is clearly shown in beetle. For better illustrating this property, we can take a close look at the mesh (Figure 5).</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_5.jpg" alt="tutte embedding for parameterization figure 5"></p><center><b>Figure 5:</b> Parts of parameterizations of cow.</center><p>Figure 5 takes the same 200×200 pixels parts from the screen shots of the parametrizations of cow object. It shows that uniform Laplacian weight can cause the triangles on the parameterization to become more regular, but the triangles on the parameterized mesh of Laplace-Beltrami weight and mean value weight are more similar to those on the original mesh.</p><p>Besides, Laplace-Beltrami weight and mean value weight can even better preserve the patterns on the mesh. As shown in Figure 6, the features on the face object (i.e., mouth and eyes) are more distinguishable if we use Laplace-Beltrami weight or mean value weight.</p><p><img src="/img/tutte_embedding_for_parameterization/tutte_embedding_for_parameterization_6.jpg" alt="tutte embedding for parameterization figure 6"></p><center><b>Figure 6:</b> Parameterizations of face.</center><p>I have also uploaded the project to this blog: <a href="/projects/tutte-embedding">/projects/tutte-embedding</a>, we can go there to see how the algorithms work.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;In terms of geometry processing, &lt;a href=&quot;https://en.wikipedia.org/wiki/Tutte_embedding&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Tutte embedding&lt;/
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="parameterization" scheme="https://isaacguan.github.io/tags/parameterization/"/>
    
      <category term="tutte-embedding" scheme="https://isaacguan.github.io/tags/tutte-embedding/"/>
    
  </entry>
  
  <entry>
    <title>Hough Transform for Plane Detection</title>
    <link href="https://isaacguan.github.io/2018/02/28/Hough-Transform-for-Plane-Detection/"/>
    <id>https://isaacguan.github.io/2018/02/28/Hough-Transform-for-Plane-Detection/</id>
    <published>2018-03-01T02:02:15.000Z</published>
    <updated>2018-03-19T23:45:12.888Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Hough_transform" target="_blank" rel="external">Hough transform</a> is a method for detecting parameterized objects, typically used for lines and circles in 2D space. Nowadays, with proliferation of acquisitive devices, deriving a massive point cloud is an easy task. As we can also parameterize objects in 3D, Hough transform can be applied to detect planes in 3D point clouds as well.</p><h1 id="Parameterization-of-a-Plane"><a href="#Parameterization-of-a-Plane" class="headerlink" title="Parameterization of a Plane"></a>Parameterization of a Plane</h1><p>Similar to a line in 2D space, a plane in 3D can be described using a <a href="https://en.wikipedia.org/wiki/Linear_equation#Slope%E2%80%93intercept_form" target="_blank" rel="external">slope–intercept equation</a>, where $k_x$ is the slope in x-direction, $k_y$ is the slope in y-direction, and $b$ is the intercept on z-axis:$$\begin{equation}z=k_xx+k_yy+b.\tag{1}\label{eq:1}\end{equation}$$</p><p>With $\eqref{eq:1}$ we can simply parameterize a plane as $(k_x,k_y,b)$.</p><p>However, values of $k_x$, $k_y$, and $b$ are unbounded, which would pose a problem when we try to parameterize a vertical plane. Thus, for computational reasons, we can write the plane equation in the <a href="https://en.wikipedia.org/wiki/Hesse_normal_form" target="_blank" rel="external">Hesse normal form</a>, where $\theta$ is the angle between the normal vector of this plane and the x-axis, $\phi$ is the angle between the normal vector and z-axis, and $\rho$ is the distance from the plane to the origin:$$\begin{equation}x\cos\theta\sin\phi+y\sin\phi\sin\theta+z\cos\phi=\rho.\tag{2}\label{eq:2}\end{equation}$$</p><p>As shown in Figure 1. The plane can be parameterized as $(\theta,\phi,\rho)$.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_1.jpg" alt="hough transform for plane detection figure 1"></p><center><b>Figure 1:</b> Parameterization of a plane.</center><p>To find planes in a 3D point cloud, we have to calculate the Hough transform for each point, which is to say that we parameterize every possible plane that go through every point in the $(\theta,\phi,\rho)$ Hough space. For instance, Figure 2 shows the parameterization of three points $(0,0,1)$, $(0,1,0)$, and $(1,0,0)$. Each point is marked as a 3D sinusoid curve in Hough space and the intersection which is marked in black denotes the plane defined by the three points.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_2.jpg" alt="hough transform for plane detection figure 2"></p><center><b>Figure 2:</b> Transformation of three points from original space to Hough space.</center><h1 id="Hough-Methods"><a href="#Hough-Methods" class="headerlink" title="Hough Methods"></a>Hough Methods</h1><p>Basically, the algorithm for doing Hough transform can be described as a voting method, where we discretize the Hough space with a bunch of $(\theta,\phi,\rho)$ cells. A data structure called accumulator then is needed to store all these cells with a score parameter for every cell. Incrementing a cell means increasing the score of it by +1. Each point votes for all cells of $(\theta,\phi,\rho)$ that define a plane on which it may lie.</p><h2 id="Standard-Hough-Transform"><a href="#Standard-Hough-Transform" class="headerlink" title="Standard Hough Transform"></a>Standard Hough Transform</h2><p>A most basic and naïve Hough transform algorithm for plane detection is outlined as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Traverse all the points in the point cloud $P$;</p><p><em>Step 2:</em> For each point $\mathbf{p}_i$ in $P$, vote for all the $A(\theta,\phi,\rho)$ cells in the accumulator $A$ defined by this point;</p><p><em>Step 3:</em> After the whole iteration, search for the most prominent cells in the accumulator $A$, that define the detected planes in $P$.</p><p><strong><em>End</em></strong></p><p>The standard Hough transform is performed in two stages: incrementing the cells, which needs $O(|P|\cdot N_\phi\cdot N_\theta)$ operations, and searching for the most prominent cells, which takes $O(N_\rho\cdot N_\phi\cdot N_\theta)$ time, where $|P|$ is the size of the point cloud, $N_\phi$ is the number of cells in direction of $\phi$, $N_\theta$ in direction of $\theta$, and $N_\rho$ in direction of $\rho$.</p><h2 id="Probabilistic-Hough-Transform"><a href="#Probabilistic-Hough-Transform" class="headerlink" title="Probabilistic Hough Transform"></a>Probabilistic Hough Transform</h2><p>In the standard Hough transform, the size of the point cloud $|P|$ is usually much larger than the number $N_\rho \cdot N_\phi \cdot N_\theta$ of cells in the accumulator array. We can simply reduce the number of points to improve the computational expenses. Thus, the standard Hough transform can be adapted to the probabilistic Hough transform, which is shown as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Determine the size value $m$ and the threshold value $t$;</p><p><em>Step 2:</em> randomly select $m$ points to create $P^m\subset P$;</p><p><em>Step 3:</em> Do the standard Hough transform on the point set $P^m$;</p><p><em>Step 4:</em> Delete the cells from the accumulator $A$ with a value that does not reach $t$, and search for the most prominent cells in $A$, that define the detected planes in $P$.</p><p><strong><em>End</em></strong></p><p>The $m$ points (with $m&lt;|P|$) are randomly selected from the point cloud $P$, so the dominant part of the runtime is proportional to $O(m\cdot N_\phi\cdot N_\theta)$. However, the optimal choice of $m$ and the threshold $t$ depends on the actual problem, e.g., the number of planes, or the noise in the point cloud.</p><h3 id="Adaptive-Probabilistic-Hough-Transform"><a href="#Adaptive-Probabilistic-Hough-Transform" class="headerlink" title="Adaptive Probabilistic Hough Transform"></a>Adaptive Probabilistic Hough Transform</h3><p>In order to find the optimal subsample of the point cloud, we can use an adaptive method to determine the reasonable number of selected points. The adaptive probabilistic Hough transform monitors the accumulator. The structure of the accumulator changes dynamically during the voting phase. As soon as stable structures emerge and turn into significant peaks, voting is terminated.</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Check the stability order $m_k$ of the list of $k$ peaks $S_k$ in the accumulator, if it reaches the threshold value $t_k$ then finish;</p><p><em>Step 2:</em> Randomly select a small subset $P^m\subset P$;</p><p><em>Step 3:</em> Do the standard Hough transform on the point set $P^m$;</p><p><em>Step 4:</em> Merge the active list of peaks $S_k$ with the previous one, determine the stability order $m_k$, goto <em>Step 1</em>;</p><p><strong><em>End</em></strong></p><p>In this algorithm, The stability is described by a set $S_k$ of $k$ peaks in the list, if the set contains all largest peaks before and after one update phase. The number $m_k$ of consecutive lists in which $S_k$ is stable is called the stability order of $S_k$.</p><h3 id="Progressive-Probabilistic-Hough-Transform"><a href="#Progressive-Probabilistic-Hough-Transform" class="headerlink" title="Progressive Probabilistic Hough Transform"></a>Progressive Probabilistic Hough Transform</h3><p>The progressive probabilistic Hough transform calculates stopping time for random selection of points. The algorithm stops whenever a cell count exceeds a threshold.</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Check the input point cloud $P$, if it is empty then finish;</p><p><em>Step 2:</em> Update the accumulator with a single point $\mathbf{p}_i$ randomly selected from $P$;</p><p><em>Step 3:</em> Remove $\mathbf{p}_i$ from $P$ and add it to $P_\text{voted}$;</p><p><em>Step 4:</em> Check if the highest peak in the accumulator that was modified by the new point is higher than threshold $t$, if not then goto <em>Step 1</em>;</p><p><em>Step 5:</em> Select all points from $P$ and $P_\text{voted}$ that are close to the plane defined by the highest peak and add them to $P_\text{plane}$;</p><p><em>Step 6:</em> Search for the largest connected region $P_\text{region}$ in $P_\text{plane}$ and remove from $P$ all points in $P_\text{region}$;</p><p><em>Step 7:</em> Reset the accumulator by unvoting all the points in $P_\text{region}$;</p><p><em>Step 8:</em> If the area covered by $P_\text{region}$ is larger than a threshold, add it to the output list, goto <em>Step 1</em>;</p><p><strong><em>End</em></strong></p><p>In this algorithm, $P_\text{voted}$ is the point set of all the voted points before a plane is detected, $P_\text{plane}$ is the set of points in the detected planes, and $P_\text{region}$ denotes the largest connected region in $P_\text{plane}$. For determining the stopping time, threshold $t$ is predicted on the percentage of votes for one cell from all points that have voted.</p><h2 id="Randomized-Hough-Transform"><a href="#Randomized-Hough-Transform" class="headerlink" title="Randomized Hough Transform"></a>Randomized Hough Transform</h2><p>As we know the fact that a plane is defined by three points. For detecting planes, three points from the input space are mapped onto one point in the Hough space. When a plane is represented by a large number of points, it is more likely that three points from this plane are randomly selected. Eventually the cells corresponding to actual planes receive more votes and are distinguishable from the other cells. Inspired by this idea, we can come up with an algorithm described as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Check the input point cloud $P$, if it is empty then finish;</p><p><em>Step 2:</em> Randomly pick three points $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$ from $P$;</p><p><em>Step 3:</em> If $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$ fulfill the distance criterion, calculate plane $(\theta,\phi,\rho)$ spanned by $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$, and increment cell $A(\theta,\phi,\rho)$ in the accumulator space;</p><p><em>Step 4:</em> If the count of $|A(\theta,\phi,\rho)|$ reaches threshold $t$, $(\theta,\phi,\rho)$ parameterize the detected plane, delete all points close to $(\theta,\phi,\rho)$ from $P$, and reset the accumulator $A$;</p><p><em>Step 5:</em> Goto Step 1;</p><p><strong><em>End</em></strong></p><p>This algorithm simply decreases the number of cells touched by exploiting the fact that a curve with $n$ parameters is defined by $n$ points. And also note that, if points are very far apart, they most likely do not belong to one plane. To take care of this and to diminish errors from sensor noise a distance criterion is introduced: $\mathrm{dist}(\mathbf{p}_1,\mathbf{p}_2,\mathbf{p}_3)\leq\mathrm{dist}_\text{max}$, i.e., the maximum point-to-point distance between $\mathbf{p}_1$, $\mathbf{p}_2$, and $\mathbf{p}_3$ is below a fixed threshold.</p><h1 id="Accumulator"><a href="#Accumulator" class="headerlink" title="Accumulator"></a>Accumulator</h1><p>An inappropriate accumulator may lead to detection failures of some specific planes and difficulties in finding local maxima, displays low accuracy, large storage space, and low speed. A tradeoff has to be found between a coarse discretization that accurately detects planes and a small number of cells in the accumulator to decrease the time needed for the Hough transform.</p><h2 id="Accumulator-Array"><a href="#Accumulator-Array" class="headerlink" title="Accumulator Array"></a>Accumulator Array</h2><p>For the standard implementation of the 2D Hough transform, the Hough space is divided into $N_\rho\times N_\theta$ rectangular cells. The size of the cells is variable and is chosen problem dependent. Using the same subdivision for the 3D Hough space by dividing it into cuboid cells results in the patches seen in Figure 3. The cells closer to the poles are smaller and comprise less normal vectors. This means voting favors the larger equatorial cells.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_3.jpg" alt="hough transform for plane detection figure 3"></p><center><b>Figure 3:</b> Accumulator array.</center><h2 id="Accumulator-Cube"><a href="#Accumulator-Cube" class="headerlink" title="Accumulator Cube"></a>Accumulator Cube</h2><p>We can also project the unit sphere $S^2$ onto the smallest cube that contains the sphere using the diffeomorphism:$$\begin{equation}\phi:S^2\to\mathrm{cube},s\mapsto\|s\|_\infty.\tag{3}\label{eq:3}\end{equation}$$</p><p>Each face of the cube is divided into a regular grid, which is shown in Figure 4. With this design of accumulator, the difference of size between the patches on the unit sphere is negligible.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_4.jpg" alt="hough transform for plane detection figure 4"></p><center><b>Figure 4:</b> Accumulator cube.</center><h2 id="Accumulator-Ball"><a href="#Accumulator-Ball" class="headerlink" title="Accumulator Ball"></a>Accumulator Ball</h2><p>The commonly used design, the accumulator array, which is shown in Figure 3, causes the irregularity between the patches on the unit sphere. To solve this issue, we can simply customize the the resolution in terms of polar coordinates depending on the position of the sphere. The resolution of the longitude $\phi$ is kept as for the accumulator array, which is defined as $\phi^\prime$. In $\theta$ direction, the largest latitude circle is the equator located at $\phi=0$. For the unit sphere it has the $l_\text{max}=2\pi$. The length of the latitude circle in the middle of the segment located above $\phi_i$ is given by $l_i=2\pi(\phi+\phi^\prime)$. The step width in $\theta$ direction for each slice is now computed as $\theta_{\phi_i}=\frac{360^\circ\cdot l_\text{max}}{l_i\cdot N_\theta}$, where $N_\theta$ is a constant that can be customized. The resulting design is illustrated in Figure 5.</p><p><img src="/img/hough_transform_for_plane_detection/hough_transform_for_plane_detection_5.jpg" alt="hough transform for plane detection figure 5"></p><center><b>Figure 5:</b> Accumulator ball.</center>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Hough_transform&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Hough transform&lt;/a&gt; is a method for detecting 
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="hough-transform" scheme="https://isaacguan.github.io/tags/hough-transform/"/>
    
      <category term="parameterization" scheme="https://isaacguan.github.io/tags/parameterization/"/>
    
      <category term="plane-detection" scheme="https://isaacguan.github.io/tags/plane-detection/"/>
    
  </entry>
  
  <entry>
    <title>Mesh Smoothing</title>
    <link href="https://isaacguan.github.io/2018/01/26/Mesh-Smoothing/"/>
    <id>https://isaacguan.github.io/2018/01/26/Mesh-Smoothing/</id>
    <published>2018-01-26T17:45:19.000Z</published>
    <updated>2018-02-06T18:10:00.138Z</updated>
    
    <content type="html"><![CDATA[<p>Mesh smoothing, also known as mesh denoising, is an important and widely discussed topic in terms of geometry processing. Basically, a mesh smoothing method takes three steps: loading a mesh from a file; smoothing that mesh; outputting the mesh to a file. Recently, I implemented a few basic algorithms regarding mesh smoothing.</p><h1 id="Selection-of-Mesh-Library"><a href="#Selection-of-Mesh-Library" class="headerlink" title="Selection of Mesh Library"></a>Selection of Mesh Library</h1><p>There are a lot of mesh libraries that give us a data structure so that we can use to build a mesh for an object file. What I am using is the <a href="https://github.com/GeometryCollective/geometry-processing-js" target="_blank" rel="external">geometry-processing-js</a> library. It is a library written in JavaScript which constructs a mesh based on <a href="https://en.wikipedia.org/wiki/Doubly_connected_edge_list" target="_blank" rel="external">half-edge</a>. It also gives plenty of predefined functions regarding the components of a mesh e.g. half-edge, vertex, edge, face, etc. Besides, it also includes a linear algebra library which makes it handy for users to do matrix-vector operations.</p><h1 id="Mesh-Smoothing-Algorithms"><a href="#Mesh-Smoothing-Algorithms" class="headerlink" title="Mesh Smoothing Algorithms"></a>Mesh Smoothing Algorithms</h1><p>I implemented two algorithms for mesh smoothing, a mean filtering algorithm, and a weighted mean filtering algorithm.</p><h2 id="Mean-Filtering-Algorithm"><a href="#Mean-Filtering-Algorithm" class="headerlink" title="Mean Filtering Algorithm"></a>Mean Filtering Algorithm</h2><p>A most naïve mean filtering algorithm goes as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Create a hash map $\mathrm{positions}$ mapping each vertex with its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, calculate $\mathrm{positions}[\mathbf{v}]$ by the average position of the neighbor vertices in the one-ring neighborhood of $\mathbf{v}$;</p><p><em>Step 3:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><p>However, if the object has some holes on it, this algorithm could cause the holes to enlarge after times of iteration, which is shown in Figure 1.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_1.jpg" alt="mesh smoothing figure 1"></p><center><b>Figure 1:</b> Holes enlarge on the object, steps = 5.</center><p>Sometimes, we don’t like such distortion, so it is necessary to preserve the boundary of holes on the mesh. We can add a step to the algorithm to constrain the vertices of boundary edges:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Create a hash map $\mathrm{positions}$ mapping each vertex with its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, calculate $\mathrm{positions}[\mathbf{v}]$ by the average position of the neighbor vertices in the one-ring neighborhood of $\mathbf{v}$;</p><p><em>Step 3:</em> To preserve the boundary of holes on the mesh, if vertex $\mathbf{v}$ is on boundary, update $\mathrm{positions}[\mathbf{v}]$ to the original position of vertex $\mathbf{v}$ in the mesh;</p><p><em>Step 4:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><h2 id="Weighted-Mean-Filtering-Algorithm"><a href="#Weighted-Mean-Filtering-Algorithm" class="headerlink" title="Weighted Mean Filtering Algorithm"></a>Weighted Mean Filtering Algorithm</h2><p>The weighting method simply weights each neighboring vertex by the area of the two faces incident to it. The weighted mean filtering algorithm goes like this:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em>  Create a hash map $\mathrm{positions}$ that maps each vertex to its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, traverse its neighborhood vertices. And for each neighbor vertex $\mathbf{v}^\ast$ calculate the weight value $w^\ast$ on each neighborhood vertex by adding area of the two incident faces together;</p><p><em>Step 3:</em> After the traversal of neighborhood vertices, calculate the total weight $w$ on the vertex $\mathbf{v}$ by accumulating every $w^\ast$, and calculate $\mathrm{positions}[\mathbf{v}]=\frac{\sum_{}w^\ast\cdot\mathbf{v}^\ast}{w}$;</p><p><em>Step 4:</em> To preserve the boundary of holes on the mesh, if vertex $\mathbf{v}$ is on boundary, update $\mathrm{positions}[\mathbf{v}]$ to the original position of vertex $\mathbf{v}$ in the mesh;</p><p><em>Step 5:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><p>However, in the implementation, I found this could cause the faces of the mesh overlapping one another after several times of iterations, because the position of vertex could fall outside its one-ring neighborhood. As shown in Figure 2, overlapping occurs after 5 times of iteration.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_2.jpg" alt="mesh smoothing figure 2"></p><center><b>Figure 2:</b> Overlapping of faces, steps = 5.</center><p>In order to prevent such thing from happening, I tried to find those vertices which could fall outside its neighborhood during an iteration. The one-ring neighborhood of one vertex is actually a star-shaped polygon so that we can use the left-turn/right-turn check to decide whether the vertex is inside it or not.</p><p>To find out the star-shaped neighborhood of a certain vertex $\mathbf{v}$, we should first project all the neighbors onto the plane which is defined by vertex $\mathbf{v}$ and its normal vector $\mathbf{n}$. We can find the projection $\mathbf{q}=(x^\prime,y^\prime,z^\prime)$ of vertex $\mathbf{p}=(x,y,z)$ on the plane that defined by the normal $\mathbf{n}=(a,b,c)$ and vertex $\mathbf{v}=(x_0,y_0,z_0)$ in the way below.</p><p>There are following implicit geometric relationships between $\mathbf{p}$, $\mathbf{q}$, $\mathbf{n}$, and $\mathbf{v}$: $(\mathbf{q}-\mathbf{p})\parallel\mathbf{n}$ and $(\mathbf{q}-\mathbf{v})\perp\mathbf{n}$.</p><p>The following equation can be derived from $(\mathbf{q}-\mathbf{p})\parallel\mathbf{n}$:$$\begin{equation}\frac{x^\prime-x}{a}=\frac{y^\prime-y}{b}=\frac{z^\prime-z}{c}=t.\tag{1}\label{eq:1}\end{equation}$$</p><p>And from $(\mathbf{q}-\mathbf{v})\perp\mathbf{n}$, we can derive:$$\begin{equation}a(x^\prime-x_0)+b(y^\prime-y_0)+c(z^\prime-z_0)=0.\tag{2}\label{eq:2}\end{equation}$$</p><p>Combining $\eqref{eq:1}$ and $\eqref{eq:2}$, we can solve for $t$:$$\begin{equation}t=\frac{ax_0+by_0+cz_0-(ax+by+cz)}{a^2+b^2+c^2}.\tag{3}\label{eq:3}\end{equation}$$</p><p>Then we can solve for the projection $\mathbf{q}=(x^\prime,y^\prime,z^\prime)$ by combining $\eqref{eq:1}$ and $\eqref{eq:3}$.</p><p>As the data structure of mesh is based on half-edge, and all the faces are defined by half-edges in counter-clock-wise order. Thus, for each vertex $\mathbf{v}$, we can simply get the boundary edges of a star-shaped neighborhood in counter-clock-wise order, project them along with the position to be updated $\mathrm{positions}[\mathbf{v}]$ onto the plane that is defined by vertex $\mathbf{v}$ and its normal vector $\mathbf{n}$. Then we can do the left-turn/right-turn check on that plane to see whether the new position for vertex $\mathbf{v}$ $\mathrm{positions}[\mathbf{v}]$ is outside its neighborhood polygon or not.</p><p>So the modified algorithm goes as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Create a hash map $\mathrm{positions}$ that maps each vertex to its position to be updated;</p><p><em>Step 2:</em> For each vertex $\mathbf{v}$ in the mesh, traverse its neighborhood vertices. And for each neighbor vertex $\mathbf{v}^\ast$ calculate the weight value $w^\ast$ on each neighborhood vertex by adding area of the two incident faces together;</p><p><em>Step 3:</em> After the traversal of neighborhood vertices, calculate the total weight $w$ on the vertex $\mathbf{v}$ by accumulating every $w^\ast$, and calculate $\mathrm{positions}[\mathbf{v}]=\frac{\sum_{}w^\ast\cdot\mathbf{v}^\ast}{w}$;</p><p><em>Step 4:</em> Check whether $\mathrm{positions}[\mathbf{v}]$ falls outside its one-ring neighborhood, if so, use the mean filtering algorithm to update the new position;</p><p><em>Step 5:</em> To preserve the boundary of holes on the mesh, if vertex $\mathbf{v}$ is on boundary, update $\mathrm{positions}[\mathbf{v}]$ to the original position of vertex $\mathbf{v}$ in the mesh;</p><p><em>Step 6:</em> After the whole iteration, update the positions of vertices in the mesh by the hash map $\mathrm{positions}$.</p><p><strong><em>End</em></strong></p><h1 id="Results-and-Conclusion"><a href="#Results-and-Conclusion" class="headerlink" title="Results and Conclusion"></a>Results and Conclusion</h1><p>As for the mean filtering algorithm, Figure 3 shows the results for after applying it on different objects with different step numbers.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_3.jpg" alt="mesh smoothing figure 3"></p><center><b>Figure 3:</b> Results of mean filtering algorithm.</center><p>As for the weighted mean filtering algorithm, Figure 4 shows the results for after applying it on different objects with different step numbers.</p><p><img src="/img/mesh_smoothing/mesh_smoothing_4.jpg" alt="mesh smoothing figure 4"></p><center><b>Figure 4:</b> Results of weighted mean filtering algorithm.</center><p>We can conclude that the results of both algorithms basically look alike, and both of them could cause the volume of object to shrink if we apply too many times.</p><p>I have also uploaded the project to this blog: <a href="/projects/mesh-smoothing">/projects/mesh-smoothing</a>, we can go there to see how exactly the algorithms work.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Mesh smoothing, also known as mesh denoising, is an important and widely discussed topic in terms of geometry processing. Basically, a me
      
    
    </summary>
    
    
      <category term="geometry-processing" scheme="https://isaacguan.github.io/tags/geometry-processing/"/>
    
      <category term="mesh-smoothing" scheme="https://isaacguan.github.io/tags/mesh-smoothing/"/>
    
  </entry>
  
  <entry>
    <title>Implementation of Voronoi Diagram and Delaunay Triangulation</title>
    <link href="https://isaacguan.github.io/2017/12/22/Implementation-of-Voronoi-Diagram-and-Delaunay-Triangulation/"/>
    <id>https://isaacguan.github.io/2017/12/22/Implementation-of-Voronoi-Diagram-and-Delaunay-Triangulation/</id>
    <published>2017-12-23T01:28:50.000Z</published>
    <updated>2017-12-24T02:50:28.956Z</updated>
    
    <content type="html"><![CDATA[<p>The <a href="https://en.wikipedia.org/wiki/Voronoi_diagram" target="_blank" rel="external">Voronoi diagram</a> and the <a href="https://en.wikipedia.org/wiki/Delaunay_triangulation" target="_blank" rel="external">Delaunay triangulation</a> are dual representations of a set of points to each other. Due to their wide applications in science and technology, the Voronoi diagram and the Delaunay triangulation play important roles in the field of computational geometry. In this post, I am going to introduce an implementation of an algorithm to derive both the Voronoi diagram and the Delaunay triangulation of a set of points in the plane.</p><h1 id="Voronoi-Diagram-and-Delaunay-Triangulation"><a href="#Voronoi-Diagram-and-Delaunay-Triangulation" class="headerlink" title="Voronoi Diagram and Delaunay Triangulation"></a>Voronoi Diagram and Delaunay Triangulation</h1><p>The Voronoi diagram of a set of points, also known as Thiessen polygons, is a partitioning of a plane into regions by a set of continuous polygons consisting of perpendicular bisectors of the connecting lines of two adjacent points. These regions are called Voronoi cells. And for each point in the set, there is a corresponding Voronoi cell consists of all points closer to that point than to any other.</p><p>The Delaunay triangulation of a set of points is dual to its Voronoi diagram. It is a collection of connected but non-overlapping triangles, and the outer circumcircle of these triangles does not contain any other points in this set.</p><h1 id="Design-of-the-Algorithm"><a href="#Design-of-the-Algorithm" class="headerlink" title="Design of the Algorithm"></a>Design of the Algorithm</h1><p>There are a lot of ways to generate a Voronoi diagram from a set of points in the plane. In this implementation, I obtained the Voronoi diagram from generating its dual, the Delaunay triangulation. Generally speaking, for the set of $n$ points $P=\{\mathbf{p}_1,\mathbf{p}_2,\ldots,\mathbf{p}_n\}$ in $\mathbb{R}^2$, the algorithm goes in this way: the Delaunay triangulation of the set of points is firstly generated, then we calculate the center of the circumcircle of each triangle, and finally we connect these centers with straight lines and form the polygon mesh generated from the vertices of the triangles.</p><h2 id="Design-of-the-Data-Structure"><a href="#Design-of-the-Data-Structure" class="headerlink" title="Design of the Data Structure"></a>Design of the Data Structure</h2><p>I implemented this algorithm in object oriented language, so the design of data structure is in the form of class.</p><p>Point:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">Point</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">double</span> x, y, z;</div><div class="line">    <span class="keyword">public</span> List&lt;<span class="keyword">int</span>&gt; adjoinTriangles;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Point</span>(<span class="params"><span class="keyword">double</span> x, <span class="keyword">double</span> y, <span class="keyword">double</span> z</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.x = x;</div><div class="line">        <span class="keyword">this</span>.y = y;</div><div class="line">        <span class="keyword">this</span>.z = z;</div><div class="line">        adjoinTriangles = <span class="keyword">new</span> List&lt;<span class="keyword">int</span>&gt;();</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Voronoi edge:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">VoronoiEdge</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> Point start, end;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">VoronoiEdge</span>(<span class="params">Point start, Point end</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.start = start;</div><div class="line">        <span class="keyword">this</span>.end = end;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Delaunay edge:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">DelaunayEdge</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">int</span> start, end;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DelaunayEdge</span>(<span class="params"><span class="keyword">int</span> start, <span class="keyword">int</span> end</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.start = start;</div><div class="line">        <span class="keyword">this</span>.end = end;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Triangle:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">Triangle</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">int</span> vertex1, vertex2, vertex3;</div><div class="line">    <span class="keyword">public</span> Point center;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">double</span> radius;</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Triangle</span>(<span class="params"><span class="keyword">int</span> vertex1, <span class="keyword">int</span> vertex2, <span class="keyword">int</span> vertex3</span>)</span></div><div class="line"><span class="function">    </span>&#123;</div><div class="line">        <span class="keyword">this</span>.vertex1 = vertex1;</div><div class="line">        <span class="keyword">this</span>.vertex2 = vertex2;</div><div class="line">        <span class="keyword">this</span>.vertex3 = vertex3;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>On top of that, I defined a point list and a triangle list under the Collections class to store all the points in the point set $P$ and all the triangles during the procedure of triangulation as global variables:</p><figure class="highlight cs"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">class</span> <span class="title">Collections</span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;Point&gt; allPoints;</div><div class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;Triangle&gt; allTriangles;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>Thus, in the Point class, DelaunayEdge class and Triangle class, I can use an integer to represent the index of a certain point or triangle from the global lists, and retrieve it directly.</p><h2 id="Build-up-the-Vonoroi-Diagram"><a href="#Build-up-the-Vonoroi-Diagram" class="headerlink" title="Build up the Vonoroi Diagram"></a>Build up the Vonoroi Diagram</h2><p>The algorithm of building up a Vonoroi diagram goes in this way:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Obtain the Delaunay triangulation by generating the list of Delaunay edges.</p><p><em>Step 2:</em> Traverse all the Delaunay edges.</p><p><em>Step 3:</em> For each Delaunay edge, traverse the two triangle lists stored with the start point and the end point, and find the two same triangles in the two lists which are the adjacent triangles of this Delaunay edge.</p><p><em>Step 4:</em> Construct a Voronoi edge by connecting the two centers of the circumcles of the two adjacent triangles and add it to the Voronoi edge list.</p><p><strong><em>End</em></strong></p><h2 id="Conduct-the-Delaunay-Triangulation"><a href="#Conduct-the-Delaunay-Triangulation" class="headerlink" title="Conduct the Delaunay Triangulation"></a>Conduct the Delaunay Triangulation</h2><p>I planned to apply the <a href="https://en.wikipedia.org/wiki/Bowyer%E2%80%93Watson_algorithm" target="_blank" rel="external">Bowyer–Watson algorithm</a> for computing the Delaunay triangulation. A most naïve Bowyer–Watson algorithm goes like this:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Construct a “super” triangle that covers all the points from the point set, add it to the Delaunay triangle list. </p><p><em>Step 2:</em> Insert points from the point set $P=\{\mathbf{p}_1,\mathbf{p}_2,\ldots,\mathbf{p}_n\}$ to the “super” triangle one by one.</p><p><em>Step 3:</em> For each point $\mathbf{p}_i$ inserted, traverse the Delaunay triangle list to find all the triangles whose circumcircles cover this point $\mathbf{p}_i$ as invalid triangles, delete these triangles from the Delaunay triangle list and delete all the common edges of these triangles, and leave a star-shaped polygonal hole.</p><p><em>Step 4:</em> Connect the point $\mathbf{p}_i$ to all the vertices of this star-shaped polygon, and add the newly formed triangles to the Delaunay triangle list.</p><p><em>Step 5:</em> After all the points are inserted, obtain the Delaunay edge list from the Delaunay triangle list, and delete the edges from the Delaunay edge list that contain a vertex of the original “super” triangle.</p><p><strong><em>End</em></strong></p><p>The following pictures can better illustrate the key steps of this algorithm. As shown in Figure 1, when a new point is inserted, all the triangles whose circumcircles contain this point will be found. The common edges of these triangles, which are highlighted in yellow, will be deleted, leaving the star-shaped boundary in red.</p><p><img src="/img/implementation_of_voronoi_diagram_and_delaunay_triangulation/implementation_of_voronoi_diagram_and_delaunay_triangulation_1.jpg" alt="implementation of voronoi diagram and delaunay triangulation figure 1"></p><center><b>Figure 1:</b> A new point inserted.</center><p>And then, the inserted point will be connected to all the vertices of the star-shaped polygon, as shown in Figure 2, the new Delaunay triangles will be formed.</p><p><img src="/img/implementation_of_voronoi_diagram_and_delaunay_triangulation/implementation_of_voronoi_diagram_and_delaunay_triangulation_2.jpg" alt="implementation of voronoi diagram and delaunay triangulation figure 2"></p><center><b>Figure 2:</b> Connecting to the inserted point.</center><p>However, the aforementioned naïve manner of Delaunay triangulation is clearly an $O(n^2)$ time algorithm that is incapable of handling a massive amount of points.</p><p>For improving its efficiency, we can first sort the set of points by x-coordinate, and use an open list and a closed list to store all the Delaunay triangles. In each time a point is inserted, all the triangles with circumcircles to the left of the inserting point are put into the closed list and removed from the open list. All the new triangles generated in an insertion are put into the open list. So in each time of insertion, we just have to traverse the open list to find the invalid triangles, the length of the sequential search of triangles is much reduced.</p><p>Besides, in order to derive the Voronoi diagram, when a new point is inserted, all the newly formed triangles that are incident on the point are put into the triangle list that is stored with the point.</p><p>Thus, the optimized algorithm goes as follows:</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Sort the points in the point set $P=\{\mathbf{p}_1,\mathbf{p}_2,\ldots,\mathbf{p}_n\}$ by x-coordinate.</p><p><em>Step 2:</em> Construct a “super” triangle that covers all the points from the point set, add it to the open list. </p><p><em>Step 3:</em> Insert points from $P$ to the “super” triangle one by one in ascending order of x-coordinates.</p><p><em>Step 4:</em> For each point $\mathbf{p}_i$ inserted, traverse the open list to: 1) find all the triangles with circumcircles lying to the left of the point $\mathbf{p}_i$, delete these triangles from the open list and add them to the closed list; 2) find all the triangles with circumcircles covering this point $\mathbf{p}_i$ as invalid triangles, delete these triangles from the open list and the triangle list stored with $\mathbf{p}_i$, delete all the common edges of these triangles, leaving a star-shaped polygonal hole.</p><p><em>Step 5:</em> Connect the point $\mathbf{p}_i$ to all the vertices of this star-shaped polygon, and add the newly formed triangles to the open list and the triangle list stored with $\mathbf{p}_i$.</p><p><em>Step 6:</em> After all the points are inserted, merge the open list and the closed list into the Delaunay triangle list, obtain the Delaunay edge list from the Delaunay triangle list, and delete the edges from the Delaunay edge list that contain a vertex of the original “super” triangle.</p><p><strong><em>End</em></strong></p><h1 id="Time-Complexity-of-the-Algorithm"><a href="#Time-Complexity-of-the-Algorithm" class="headerlink" title="Time Complexity of the Algorithm"></a>Time Complexity of the Algorithm</h1><p>The optimized algorithm for Delaunay triangulation takes $O(n\log n)$ time. As Delaunay triangulation is a planar graph, the number of triangles incident on one point is constant, so the procedure of finding adjacent triangles takes constant time and the time of generating Voronoi diagram is $O(n)$. Therefore, the total running time of this algorithm is $O(n\log n)$.</p><h1 id="Implementation-of-the-Algorithm"><a href="#Implementation-of-the-Algorithm" class="headerlink" title="Implementation of the Algorithm"></a>Implementation of the Algorithm</h1><p>This algorithm is implemented in C#. Figure 3 shows the result of the implementation of this algorithm, it is capable of handling massive input of 10000 points.</p><p><img src="/img/implementation_of_voronoi_diagram_and_delaunay_triangulation/implementation_of_voronoi_diagram_and_delaunay_triangulation_3.jpg" alt="implementation of voronoi diagram and delaunay triangulation figure 3"></p><center><b>Figure 3:</b> Voronoi diagram and Delaunay triangulation for 10000 points.</center><p>The GitHub repository of this implementation is <a href="https://github.com/IsaacGuan/Voronoi-Delaunay" target="_blank" rel="external">IsaacGuan/Voronoi-Delaunay</a>.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Voronoi_diagram&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;Voronoi diagram&lt;/a&gt; and the &lt;a href=&quot;https://e
      
    
    </summary>
    
    
      <category term="computational-geometry" scheme="https://isaacguan.github.io/tags/computational-geometry/"/>
    
      <category term="voronoi-diagram" scheme="https://isaacguan.github.io/tags/voronoi-diagram/"/>
    
      <category term="delaunay-triangulation" scheme="https://isaacguan.github.io/tags/delaunay-triangulation/"/>
    
  </entry>
  
  <entry>
    <title>Width of a Set in the Plane</title>
    <link href="https://isaacguan.github.io/2017/11/19/Width-of-a-Set-in-the-Plane/"/>
    <id>https://isaacguan.github.io/2017/11/19/Width-of-a-Set-in-the-Plane/</id>
    <published>2017-11-19T15:19:12.000Z</published>
    <updated>2017-11-19T23:24:45.930Z</updated>
    
    <content type="html"><![CDATA[<p>The book <a href="http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/scan/CMU-CS-80-101.pdf" target="_blank" rel="external"><em>Geometric Transforms for Fast Geometric Algorithms</em></a> introduces an interesting algorithm from page 84 of computing the diameter of a set of points in two dimensions. Inspired by this, I come up with a solution of computing the width of a set using geometric transformation.</p><h1 id="Definition-of-the-Width-of-a-Set"><a href="#Definition-of-the-Width-of-a-Set" class="headerlink" title="Definition of the Width of a Set"></a>Definition of the Width of a Set</h1><p>Let $S$ be a set of $n$ points in $\mathbb{R}^2$. If $l$ and $l^\prime$ are two parallel lines, then the region between them is called a slab. The width of $S$ is defined to be the minimum distance between the bounding lines of any slab that contains all points of $S$.</p><h1 id="Characterization-of-the-Width"><a href="#Characterization-of-the-Width" class="headerlink" title="Characterization of the Width"></a>Characterization of the Width</h1><p>Before presenting the width-finding algorithm, I’d like first to characterize the width of a set of points, introduce some terminology and prove several theorems.</p><p>When we say that $A$ is contained in $B$, it means that every point of $A$ also lies in $B$. The bounding lines of a slab could contain a vertex or an edge of the convex hull $H(S)$ of the set of points $S$.</p><p><strong>Theorem 1:</strong> Let $l$ and $l^\prime$ be two parallel lines that define the width of $S$, both $l$ and $l^\prime$ contain a vertex of the convex hull $H(S)$ of $S$.</p><p><strong>Proof.</strong> Assume otherwise. The width is determined by parallel lines $l$ and $l^\prime$, and $l$ contains a vertex of $H(S)$, $l^\prime$ does not, as shown in Figure 1. Then there must exists a line $l^{\prime\prime}$ closer to the set of points $S$ and produces a smaller distance. Q.E.D.</p><p><img src="/img/width_of_a_set_in_the_plane/width_of_a_set_in_the_plane_1.jpg" alt="width of a set in the plane figure 1"></p><center><b>Figure 1:</b> The width of a set.</center><p><strong>Theorem 2:</strong> Let $l$ and $l^\prime$ be two parallel lines that define the width of $S$, at least one of $l$ and $l^\prime$ contain an edge of the convex hull $H(S)$ of $S$.</p><p><strong>Proof.</strong> Assume otherwise. The width is determined by parallel lines $l$ and $l^\prime$ which both pass a vertex but neither of them passes an edge. We can rotate the parallel lines in preferred direction of rotation which means we rotate $l$ and $l^\prime$ about the two vertices they pass to form new parallel lines $l_1$ and ${l_1}^\prime$, and the distance between $l_1$ and ${l_1}^\prime$ is smaller. This contradicts the assumption. Q.E.D.</p><h1 id="Computing-the-Width"><a href="#Computing-the-Width" class="headerlink" title="Computing the Width"></a>Computing the Width</h1><p>We can divide the convex hull into 2 parts: the upper hull and the lower hull, so that when one of the parallel lines of support of $S$ meets the convex hull on the upper hull, the other is on the lower hull.</p><p>According to the <a href="https://en.wikipedia.org/wiki/Linear_programming#Duality" target="_blank" rel="external">duality transformation</a>, a non-vertical line $y=kx+b$ can be transformed into the point $(k,b)$ which is formed by the slope and intercept of the line. And a point $(a,b)$ can be transformed into the line $y=k(x-a)+b$ which means the set of lines that pass through it.</p><p>Hence, we define the transform of an edge of the convex hull $H(S)$ of $S$ as the slope of the line containing that edge and the transform of a vertex of $H(S)$ as the set of slopes of lines that pass through it. I.e., the line containing an edge $y=kx+b$ can be mapped to the one dimensional point $(k)$ and the vertex that lies between two edges $y=k_1x+b_1$ and $y=k_2x+b_2$ can be mapped to the closed interval $[k_1,k_2]$. As shown in Figure 2, the convex hull is transformed into a line which consists of a upper part and a lower part.</p><p><img src="/img/width_of_a_set_in_the_plane/width_of_a_set_in_the_plane_2.jpg" alt="width of a set in the plane figure 2"></p><center><b>Figure 2:</b> The transform.</center><p>As the slopes of the edges of the convex hull are already sorted, when we transform the convex hull to a line, the upper and lower sets of points and intervals on the line are in sorted order.</p><p>As known from Theorem 1, the parallel lines of support pass through an edge and a vertex of the convex hull $H(S)$, which means the corresponding point and interval pair contained in the parallel lines must intersect on the upper and lower lines, e.g., $\mathbf{p}_5$ and $l_8$, $\mathbf{p}_6$ and $l_2$ in Figure 2.</p><p>Thus, finding the width of a set of points $S$ can be reduced to scanning the intersections of point and interval pairs on the upper and lower hulls of its convex hull $H(S)$.</p><p>To summarize, the width of a set of points $S$ can be computed in the following manner.</p><p><strong><em>Begin</em></strong></p><p><em>Step 1:</em> Construct the convex hull $H(S)$ of $S$.</p><p><em>Step 2:</em> Apply the transform to obtain two ordered sets of points and intervals.</p><p><em>Step 3:</em> Scan the sets for intersections between the intervals of one set and the points of the other, generating the corresponding vertex and edge pair. For each such pair, compute the distance between the vertex and the extended edge, and note the smallest such distance. When the scan is complete, that distance is the width.</p><p><strong><em>End</em></strong></p><h1 id="Time-Complexity"><a href="#Time-Complexity" class="headerlink" title="Time Complexity"></a>Time Complexity</h1><p>According to the <a href="https://en.wikipedia.org/wiki/Gift_wrapping_algorithm" target="_blank" rel="external">gift wrapping algorithm</a> and the <a href="https://en.wikipedia.org/wiki/Graham_scan" target="_blank" rel="external">Graham scan algorithm</a>, the running time of finding the convex hull of a set of points can be minimized to $O(n\log h)$, where $h$ is the number of points on the convex hull. And obtaining the two ordered sets and scanning the sets both takes $O(h)$ time. Therefore, finding the width of a set of points $S$ in $\mathbb{R}^2$ takes $O(n\log h)$ time.</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;The book &lt;a href=&quot;http://reports-archive.adm.cs.cmu.edu/anon/anon/usr/ftp/scan/CMU-CS-80-101.pdf&quot; target=&quot;_blank&quot; rel=&quot;external&quot;&gt;&lt;em&gt;Geom
      
    
    </summary>
    
    
      <category term="computational-geometry" scheme="https://isaacguan.github.io/tags/computational-geometry/"/>
    
      <category term="convex-hull" scheme="https://isaacguan.github.io/tags/convex-hull/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="https://isaacguan.github.io/2017/11/01/Hello-World/"/>
    <id>https://isaacguan.github.io/2017/11/01/Hello-World/</id>
    <published>2017-11-01T20:59:25.000Z</published>
    <updated>2017-11-02T03:12:45.561Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>Existence precedes essence.</p></blockquote><p>During the high school years, I once kept a diary, sort of artsy lifestyle possessed by the petty bourgeoisie. And this experience told me the truth that writing is really time consuming, especially when you are making “sentimental twaddle” on trivial things in daily life.</p><p>Time flies. I haven’t been keeping that habit for a long time and I will be 23 this month. As every young man in his 20s, I feel at a loss from time to time. Looking back on the past few years, seemingly I have gone through a lot of things and met up with a lot of people, but future still remains uncertain.</p><p>Some people of this era are distressed and tend to explore the meaning of their existence. That is why I am quoting the words of <a href="https://en.wikipedia.org/wiki/Jean-Paul_Sartre" target="_blank" rel="external">Jean-Paul Sartre</a> at the beginning of this post. It tells that life could be meaningless according to my understanding, unless you create yourself a meaning.</p><p>But I cannot say that I am an existentialist. Existentialist should be fearless, like <a href="https://en.wikipedia.org/wiki/Friedrich_Nietzsche" target="_blank" rel="external">Nietzsche</a>, who asserted that <em>God is dead</em>. I am just following the guidance of these great philosophers to add something that I believe is meaningful to my life.</p><p>Thus, I decided to build this place and record something of myself, which generally includes:</p><ul><li>Something about my study (algorithms, programming, projects I am doing, etc.)</li><li>Something about my hobby (traveling, cycling, reading, photographing, etc.)</li><li>And more possibilities…</li></ul><p>It is lucky for me to live in this digital age, such that I can easily build up this small website on my own using <a href="https://pages.github.com/" target="_blank" rel="external">Github Pages</a> as blogging platform and <a href="https://hexo.io/" target="_blank" rel="external">Hexo</a> as framework. I also would like to express my thanks to <a href="https://github.com/fi3ework" target="_blank" rel="external">fi3ework</a> who designed this beautiful theme <a href="https://github.com/fi3ework/hexo-theme-archer" target="_blank" rel="external">archer</a> that I am using.</p><p>Anyhow, this is the very first post of my blog. Wish myself happy blogging :)</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;blockquote&gt;
&lt;p&gt;Existence precedes essence.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;During the high school years, I once kept a diary, sort of artsy lifestyle 
      
    
    </summary>
    
    
      <category term="hello-world" scheme="https://isaacguan.github.io/tags/hello-world/"/>
    
  </entry>
  
</feed>
